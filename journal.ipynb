{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import rgb2hex, colorConverter\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.cluster import SpectralClustering, KMeans, AffinityPropagation\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from seaborn import heatmap\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import networkx as nx\n",
    "\n",
    "from bson.objectid import ObjectId\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import least_squares\n",
    "from scipy import interpolate\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "import sympy\n",
    "\n",
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from itertools import product as prod_itertools\n",
    "\n",
    "from pascalanalyzer import PascalData\n",
    "from pascalanalyzer.pascalmodel import (create_model, LeastSquaresOptmizer, PascalModel)\n",
    "from profiler import Analyser\n",
    "import plotdata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from timeline import *\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database\n",
    "\n",
    "The database has multiples trained models as well as the measurements\n",
    "\n",
    "## Model fitting\n",
    "\n",
    "### Model type\n",
    "- SVR\n",
    "- Equation\n",
    "\n",
    "### Split type\n",
    "- Random\n",
    "- Halton\n",
    "\n",
    "### Number of training points\n",
    "- 10,20,..,100,all\n",
    "\n",
    "\n",
    "Each model was fitted 10 times varying the split type and number of points\n",
    "The measurements unities are GHz, KJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use('seaborn')\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 76\n",
    "plt.rcParams[\"axes.titlesize\"] = 26\n",
    "plt.rcParams[\"font.size\"] = 26\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 24\n",
    "plt.rcParams[\"ytick.labelsize\"] = 24\n",
    "plt.rcParams[\"axes.labelsize\"] = 24\n",
    "\n",
    "def matplotlib_rc_1():\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,9)\n",
    "    \n",
    "def matplotlib_rc_2():\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "matplotlib_rc_1()\n",
    "\n",
    "Path(\"experiments/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"fingerprint/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"fingerprint/phases/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"cache/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/metrics\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/power/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/energy/power\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/energy/freq_cores\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/energy/freq_inps\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/energy/cores_inps\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/overhead\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/overhead/lowest_mpe\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/analisys\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/hypothesis/const_intructions/freq\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/hypothesis/const_intructions/cores\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/hypothesis/input_instructions/input_time\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/hypothesis/input_instructions/fp\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"phases/manual\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"phases/openmp\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"phases/fingerprint\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"phases/signals\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"phases/critical_points\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAPL_ENERGY_PKG = 2.3283064365386962890625e-10\n",
    "flat = lambda x: [b for a in x for b in a]\n",
    "client = pymongo.MongoClient(\"mongodb://172.17.0.2/\")\n",
    "energydb = client[\"energy\"]\n",
    "df_times = None\n",
    "df_models = None\n",
    "df_resum = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy per instruction\n",
    "## Energy breakdown\n",
    "\n",
    "### Assembly code\n",
    "```python\n",
    "xor rcx, rcx\n",
    "mov rax, 1\n",
    "mov rdx, 0\n",
    "\n",
    "loop:\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    targ_inst(*arg)\n",
    "    \n",
    "add rcx, 1\n",
    "cmp rcx, 9999999\n",
    "jne loop\n",
    "```\n",
    "\n",
    "$E=9999999(\\frac{10}{13}inst+\\frac{3}{13}loop)$\n",
    "\n",
    "$E=7692306*inst+2307692*loop$\n",
    "\n",
    "$E=7692306*inst+constant$\n",
    "\n",
    "$E_{joules}=RAPL\\_ENERGY\\_PKG*2.3283064365386962890625e^{-10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic x86 energy breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_generic = pd.read_csv(\"databases/general_purpose.csv\").sort_values(\"energy\")\n",
    "df_generic[\"energy\"] *= RAPL_ENERGY_PKG\n",
    "print(df_generic[\"inst\"].unique())\n",
    "\n",
    "matplotlib_rc_1()\n",
    "pd.crosstab(df_generic[\"inst\"],df_generic[\"args\"],df_generic[\"energy\"],aggfunc=max).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "legend(fontsize= 14)\n",
    "title(\"Energy per argument\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_en_args_generic.pdf\")\n",
    "show()\n",
    "\n",
    "df_generic.sort_values(\"energy\").plot.bar(x=\"inst\",y=\"energy\",logy=True,legend=False)\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy consumption\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_mean_en_generic_all.pdf\")\n",
    "show()\n",
    "\n",
    "df_generic.groupby(\"inst\").energy.mean().sort_values().plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Mean energy consumption\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_mean_en_generic.pdf\")\n",
    "show()\n",
    "\n",
    "df_generic.groupby(\"inst\").energy.apply(lambda x: x.std()/x.mean()).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy deviation\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_std_en_generic.pdf\")\n",
    "show()\n",
    "\n",
    "df_generic[~df_generic[\"args\"].str.contains(\"peach\")].groupby(\"inst\").energy.apply(lambda x: x.std()/x.mean()).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy deviation without m64\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_std_en_generic_nom64.pdf\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE x86 energy breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sse = pd.read_csv(\"databases/mmx.csv\").sort_values(\"energy\")\n",
    "df_sse[\"energy\"] *= RAPL_ENERGY_PKG\n",
    "print(df_sse[\"inst\"].unique())\n",
    "\n",
    "matplotlib_rc_1()\n",
    "\n",
    "pd.crosstab(df_sse[\"inst\"],df_sse[\"args\"],df_sse[\"energy\"],aggfunc=max).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy per argument\")\n",
    "legend(fontsize= 14)\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_en_args_sse.pdf\")\n",
    "show()\n",
    "\n",
    "df_sse.sort_values(\"energy\").plot.bar(x=\"inst\",y=\"energy\")\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy consumption\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_mean_en_sse_all.pdf\")\n",
    "show()\n",
    "\n",
    "df_sse.groupby(\"inst\").energy.mean().sort_values().plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Mean energy consumption\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_mean_en_sse.pdf\")\n",
    "show()\n",
    "\n",
    "df_sse.groupby(\"inst\").energy.apply(lambda x: x.std()/x.mean()).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy deviation\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_std_en_sse.pdf\")\n",
    "show()\n",
    "\n",
    "df_sse[~df_sse[\"args\"].str.contains(\"peach\")].groupby(\"inst\").energy.apply(lambda x: x.std()/x.mean()).plot.bar()\n",
    "xticks(fontsize= 10)\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "title(\"Energy deviation without m64\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_std_en_sse_nom64.pdf\")\n",
    "show()\n",
    "\n",
    "# df_sse[\"energy\"].plot.bar(color=cm.hsv(df_sse[\"energy\"]/df_sse[\"energy\"].max()))\n",
    "# tight_layout()\n",
    "# savefig(\"inst_std_en_generic_nom64.pdf\")\n",
    "# show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic+SSE x86 energy breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sse = pd.read_csv(\"databases/mmx.csv\").sort_values(\"energy\")\n",
    "df_sse[\"energy\"] *= RAPL_ENERGY_PKG\n",
    "\n",
    "df_generic = pd.read_csv(\"databases/general_purpose.csv\").sort_values(\"energy\")\n",
    "df_generic[\"energy\"] *= RAPL_ENERGY_PKG\n",
    "\n",
    "df_generic[\"type\"]= \"generic\"\n",
    "df_sse[\"type\"]= \"sse\"\n",
    "df_all= pd.concat( (df_generic, df_sse) )\n",
    "display(df_all.head(10))\n",
    "\n",
    "df_all_mean= df_all.copy()\n",
    "df_all_mean= df_all_mean.groupby(\"inst\").mean().reset_index()\n",
    "df_all_mean= pd.merge( df_all[[\"inst\",\"type\"]], df_all_mean ).drop_duplicates()\n",
    "df_all_mean= df_all_mean.sort_values(\"energy\")\n",
    "\n",
    "c_dict= {\"generic\":\"b\", \"sse\": \"r\"}\n",
    "l_color= list(map(lambda x: c_dict[x], df_all_mean[\"type\"]))\n",
    "\n",
    "matplotlib_rc_1()\n",
    "df_all_mean.plot.bar(x=\"inst\",y=\"energy\", color=l_color)\n",
    "red_patch = mpatches.Patch(color='red', label='sse')\n",
    "blue_patch = mpatches.Patch(color='blue', label='generic')\n",
    "legend(handles=[red_patch, blue_patch])\n",
    "title(\"Energy comparision sse generic\")\n",
    "xlabel(\"Instruction\")\n",
    "ylabel(\"Energy (J)\")\n",
    "xticks(fontsize= 10)\n",
    "legend(fontsize= 14)\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_en_cmp_sse_generic.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generic222 = pd.read_csv(\"databases/generic222.csv\")\n",
    "matplotlib_rc_2()\n",
    "for inst in df_generic222[\"inst\"].unique():\n",
    "    for arg in df_generic222[\"args\"].unique():\n",
    "        df_aux = df_generic222[ (df_generic222[\"inst\"]==inst)&(df_generic222[\"args\"]==arg)&(df_generic222[\"thr\"]==3) ]\n",
    "        df_aux[\"pw\"] = df_generic222[\"energy\"]*RAPL_ENERGY_PKG/df_generic222[\"time\"]/30\n",
    "        df_aux[\"freq\"] /= 1e6\n",
    "\n",
    "        fn= lambda x,f: x[0]*f**3+x[1]*f+x[2]\n",
    "        fne= lambda x,f,y: fn(x,f)-y\n",
    "\n",
    "        x0= [1,1,1]\n",
    "        xs= least_squares(fne,x0,args=(df_aux[\"freq\"], df_aux[\"pw\"]))\n",
    "        xs.x\n",
    "\n",
    "        frs= np.arange(1,3.4,0.1)\n",
    "        plot(frs, fn(xs.x,frs), label=\"{}_{}\".format(inst,arg))\n",
    "        #plot(df_aux[\"freq\"], df_aux[\"pw\"],\".\")\n",
    "xlabel(\"Frequency (GHz)\")\n",
    "ylabel(\"Power (W)\")\n",
    "legend(fontsize=10)\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_pw_args.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generic222 = pd.read_csv(\"databases/generic222.csv\")\n",
    "colors_list = [\"r\",\"g\",\"b\",\"y\",\"c\",\"m\",\"k\"]*2\n",
    "freqs = df_generic222[\"freq\"].unique()\n",
    "f_cmap = dict(zip(freqs, colors_list[:len(freqs)]))\n",
    "print(f_cmap)\n",
    "\n",
    "matplotlib_rc_1()\n",
    "df_generic222[\"pw\"] = df_generic222[\"energy\"]*RAPL_ENERGY_PKG/df_generic222[\"time\"]/30\n",
    "dff = df_generic222[ (df_generic222[\"thr\"]==3)&(df_generic222[\"freq\"]==3000000) ]\n",
    "dff = dff.sort_values(\"freq\", ascending=False)\n",
    "dff[\"pw\"].plot.bar(color=map(lambda x: f_cmap[x], dff[\"freq\"].values))\n",
    "#legend(handles=[mpatches.Patch(color=f_cmap[p], label=p) for p in freqs])\n",
    "xlabel(\"Instruction\")\n",
    "xticks(range(dff[\"inst\"].shape[0]),dff[\"inst\"].values)\n",
    "ylabel(\"Power (W)\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/inst_pw_generic.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"databases/power_cpufrac.pkl\",\"rb\") as f:\n",
    "    data_power_cpufrac = pickle.load(f)\n",
    "\n",
    "df_power_cpufrac= []\n",
    "for f in data_power_cpufrac:\n",
    "    for t in f['threads']:\n",
    "        for p in t[\"lpcpu\"]:\n",
    "            mv= []\n",
    "            for s in p[\"rapl\"]:\n",
    "                mv.append(s[\"sensor\"])\n",
    "            df_power_cpufrac.append([f[\"freq\"], p[\"arg\"], np.mean(mv)])\n",
    "df_power_cpufrac = pd.DataFrame(df_power_cpufrac,columns=[\"freq\",\"arg\",\"pw\"])\n",
    "\n",
    "df_power_cpufrac[\"arg\"]= df_power_cpufrac[\"arg\"].astype(str)\n",
    "matplotlib_rc_2()\n",
    "for arg in df_power_cpufrac[\"arg\"].unique():\n",
    "    df_s = df_power_cpufrac[df_power_cpufrac[\"arg\"]==arg]\n",
    "    pcpu = float(df_s[\"arg\"].iloc[0].split(',')[0][2:-1])\n",
    "    print(pcpu)\n",
    "    plot(df_s[\"freq\"].astype(float)/1e6,df_s[\"pw\"],label=pcpu)\n",
    "xlabel(\"Frequency (GHz)\")\n",
    "ylabel(\"Power (W)\")\n",
    "legend()\n",
    "tight_layout()\n",
    "savefig(\"experiments/pw_freq_load.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"databases/power_cpufrac.pkl\",\"rb\") as f:\n",
    "    data_power_cpufrac = pickle.load(f)\n",
    "\n",
    "df_power_cpufrac= []\n",
    "for f in data_power_cpufrac:\n",
    "    for t in f['threads']:\n",
    "        for p in t[\"lpcpu\"]:\n",
    "            mv= []\n",
    "            for s in p[\"rapl\"]:\n",
    "                mv.append(s[\"sensor\"])\n",
    "            df_power_cpufrac.append([f[\"freq\"], p[\"arg\"], np.mean(mv)])\n",
    "df_power_cpufrac = pd.DataFrame(df_power_cpufrac,columns=[\"freq\",\"arg\",\"pw\"])\n",
    "df_power_cpufrac[\"arg\"] = df_power_cpufrac[\"arg\"].apply(lambda x: x[0])\n",
    "\n",
    "\n",
    "fn = lambda x,f,p: x[0]*f**3*p+x[1]*f+x[2]\n",
    "err = lambda x,f,p,y: fn(x,f,p)-y\n",
    "df_power_cpufrac[\"freq\"] = df_power_cpufrac[\"freq\"].astype(float)\n",
    "df_power_cpufrac[\"arg\"] = df_power_cpufrac[\"arg\"].astype(float)\n",
    "df_power_cpufrac[\"freq\"] /= 1e6\n",
    "res = least_squares(err, [1,1,1], args=(df_power_cpufrac[\"freq\"], df_power_cpufrac[\"arg\"], df_power_cpufrac[\"pw\"]))\n",
    "res.x\n",
    "\n",
    "matplotlib_rc_2()\n",
    "for arg in df_power_cpufrac[\"arg\"].unique():\n",
    "    df_s= df_power_cpufrac[df_power_cpufrac[\"arg\"]==arg]\n",
    "    plot(df_s[\"freq\"],fn(res.x,df_s[\"freq\"],arg),label=arg)\n",
    "    plot(df_s[\"freq\"],df_s[\"pw\"],\".\",c=\"b\")\n",
    "xlabel(\"frequency GHz\")\n",
    "ylabel(\"Power (W)\")\n",
    "text(1.5,35,\"P(f,%cpu)=af^3*%cpu+bf+c\")\n",
    "legend(fontsize=20)\n",
    "tight_layout()\n",
    "savefig(\"experiments/model_pw_freq_load.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"databases/a.out_freq.dat\", \"rb\") as f:\n",
    "    a_out_freq_dat = pickle.load(f)\n",
    "\n",
    "df_freq_data = np.array([0, 0, 0])\n",
    "for thr in a_out_freq_dat[\"data\"]:\n",
    "    for freq in thr[\"threads\"]:\n",
    "        aux= {k:a_out_freq_dat[k] for k in a_out_freq_dat if k != \"data\"}\n",
    "        aux[\"data\"]= freq[\"data\"]\n",
    "        #print(aux[\"to_monitor\"])\n",
    "        x = Analyser(aux)\n",
    "\n",
    "        x.df[\"input_size\"]= x.df[\"PERF_COUNT_HW_INSTRUCTIONS\"]/x.df[\"MEM_UOPS_RETIRED:ALL_STORES\"]\n",
    "\n",
    "        x.df[\"SYSTEMWIDE:RAPL_ENERGY_PKG_acc\"] = x.df[\"SYSTEMWIDE:RAPL_ENERGY_PKG\"].cumsum()\n",
    "        x.df[\"SYSTEMWIDE:RAPL_ENERGY_CORES_acc\"] = x.df[\"SYSTEMWIDE:RAPL_ENERGY_CORES\"].cumsum()\n",
    "\n",
    "        x.df[\"SYSTEMWIDE:RAPL_ENERGY_LL\"] = x.df[\"SYSTEMWIDE:RAPL_ENERGY_PKG\"]-x.df[\"SYSTEMWIDE:RAPL_ENERGY_CORES\"]\n",
    "        x.df[\"SYSTEMWIDE:RAPL_ENERGY_LL_acc\"] = x.df[\"SYSTEMWIDE:RAPL_ENERGY_LL\"].cumsum()\n",
    "\n",
    "        x.df[\"SYSTEMWIDE:RAPL_ENERGY_percent\"] = x.df[\"SYSTEMWIDE:RAPL_ENERGY_LL_acc\"]/x.df[\"SYSTEMWIDE:RAPL_ENERGY_PKG_acc\"]\n",
    "\n",
    "        x.df[\"freq\"] = [freq[\"freq\"]]*x.df[\"SYSTEMWIDE:RAPL_ENERGY_PKG\"].shape[0]\n",
    "        df_freq_data = np.vstack((df_freq_data, \n",
    "                            x.df[ [\"freq\",\"SYSTEMWIDE:RAPL_ENERGY_PKG\", \"MEM_UOPS_RETIRED:ALL_STORES\"] ].values))\n",
    "        \n",
    "df_freq_data = df_freq_data[1:]\n",
    "df_freq_data = pd.DataFrame(df_freq_data, columns=[\"freq\",\"en_cores\",\"inst\"])\n",
    "df_freq_data[\"freq\"] = df_freq_data[\"freq\"].astype(float)/1e6\n",
    "df_freq_data[\"inst\"] = df_freq_data[\"inst\"].astype(float)/1e9\n",
    "df_freq_data[\"en_cores\"] = df_freq_data[\"en_cores\"].astype(float)*2.3283064365386962890625e-10\n",
    "df_freq_data = df_freq_data[df_freq_data[\"en_cores\"]!=0]\n",
    "\n",
    "matplotlib_rc_2()\n",
    "for f in df_freq_data[\"freq\"].unique()[:1]:\n",
    "    tp= df_freq_data[df_freq_data[\"freq\"]==f].values\n",
    "    t= np.arange(0,len(tp),1)\n",
    "\n",
    "    pcpu= 1\n",
    "    for i in range(0,len(t)-1,100):\n",
    "        plot(t[:100]*0.05,tp[i:i+100:,1], label=f\"{pcpu*100}% load\")\n",
    "        pcpu-=0.25\n",
    "\n",
    "    xlabel(\"Time (s)\")\n",
    "    ylabel(\"Power (W)\")\n",
    "    legend(loc= \"upper left\",fontsize=20)\n",
    "    tight_layout()\n",
    "    savefig(\"experiments/pw_load.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voltage and frequency relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_rel = pd.DataFrame([\n",
    "    [2.2,0.77,2.199],\n",
    "    [2.1,0.76,2.099],\n",
    "    [2.0,0.75,2.000],\n",
    "    [1.9,0.74,1.899],\n",
    "    [1.8,0.73,1.799],\n",
    "    [1.7,0.72,1.699],\n",
    "    [1.6,0.71,1.599],\n",
    "    [1.5,0.70,1.500],\n",
    "    [1.4,0.69,1.397],\n",
    "    [1.3,0.68,1.297],\n",
    "    [1.2,0.67,1.200]\n",
    "], columns= [\"freq\", \"volt\", \"aperf\"])\n",
    "\n",
    "matplotlib_rc_2()\n",
    "#plot(fv_rel[\"aperf\"], fv_rel[\"volt\"])\n",
    "plot(fv_rel[\"aperf\"], fv_rel[\"volt\"], linestyle='-', marker=\"o\")\n",
    "xlabel(\"Frequency (GHz)\")\n",
    "ylabel(\"Voltage (V)\")\n",
    "tight_layout()\n",
    "savefig(\"experiments/freq_volt_rel.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_= plt.rcParams['axes.prop_cycle'].by_key()['color']*10\n",
    "\n",
    "class Clusters(dict):\n",
    "    def _repr_html_(self):\n",
    "        html = '<table style=\"border: 0;\">'\n",
    "        for c in self:\n",
    "            hx = rgb2hex(colorConverter.to_rgb(c))\n",
    "            html += '<tr style=\"border: 0;\">' \\\n",
    "            '<td style=\"background-color: {0}; ' \\\n",
    "                       'border: 0;\">' \\\n",
    "            '<code style=\"background-color: {0};\">'.format(hx)\n",
    "            html += c + '</code></td>'\n",
    "            html += '<td style=\"border: 0\"><code>' \n",
    "            html += repr(self[c]) + '</code>'\n",
    "            html += '</td></tr>'\n",
    "\n",
    "        html += '</table>'\n",
    "\n",
    "        return html\n",
    "    \n",
    "def plot_corr(df):\n",
    "    # Compute the correlation matrix for the received dataframe\n",
    "    corr = df.corr()\n",
    "    \n",
    "    # Plot the correlation matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(corr, cmap='RdYlGn')\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    \n",
    "    # Add the colorbar legend\n",
    "    cbar = fig.colorbar(cax, ticks=[-1, 0, 1], aspect=40, shrink=.8)\n",
    "\n",
    "def draw_clusters(clusters):\n",
    "    colors_aux = colors_[:]\n",
    "    html = '<table style=\"border: 0;\">'\n",
    "    for k, c in clusters.items():\n",
    "        hx = rgb2hex(colors_aux.pop()) # rgb2hex(colorConverter.to_rgb(colors_aux.pop()))\n",
    "        html += '<tr style=\"border: 0;\">' \\\n",
    "        '<td style=\"background-color: {0}; ' \\\n",
    "                   'border: 0;\">' \\\n",
    "        '<code style=\"background-color: {0};\">'.format(hx)\n",
    "        html += repr(k) + '</code></td>'\n",
    "        html += '<td style=\"border: 0\"><code>' \n",
    "        html += repr(c) + '</code>'\n",
    "        html += '</td></tr>'\n",
    "\n",
    "    html += '</table>'\n",
    "    display(HTML(html))\n",
    "\n",
    "def draw_communities(G, membership, labels):\n",
    "    matplotlib_rc_2()\n",
    "    pos= nx.spring_layout(G)\n",
    "    fig, ax = plt.subplots()\n",
    "    club_dict = defaultdict(list)\n",
    "    for student, club in enumerate(membership):\n",
    "        club_dict[club].append(student)\n",
    "    norm = colors.Normalize(vmin=0, vmax=len(club_dict.keys()))\n",
    "    colors_aux = colors_[:]\n",
    "    marks= ['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X']\n",
    "    for club, members in club_dict.items():        \n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                               nodelist=members,\n",
    "                               #node_color=cm.jet(norm(club)),\n",
    "                               node_shape=marks.pop(),\n",
    "                               node_color=[colors_aux.pop()],\n",
    "                               node_size=500,\n",
    "                               alpha=0.9,\n",
    "                               ax=ax)\n",
    "    labels= labels.str.replace('_DATASET_mem.dat','')\n",
    "    labels= labels.str.replace('EXTRALARGE','3')\n",
    "    labels= labels.str.replace('LARGE','2')\n",
    "    labels= labels.str.replace('MEDIUM','1')\n",
    "    labels= dict(enumerate(labels))\n",
    "    for p in pos:\n",
    "        pos[p]+=[0,0.03]\n",
    "    nx.draw_networkx_labels(G, pos, labels= labels, font_size=5)\n",
    "\n",
    "def plot_dendogram(Z, labels_):\n",
    "    matplotlib_rc_1()\n",
    "    figure()\n",
    "    #title('Dendrogram')\n",
    "    xlabel('Application')\n",
    "    ylabel('Distance')\n",
    "    den= sch.dendrogram(Z, leaf_rotation=90., leaf_font_size=8., labels = labels_)\n",
    "    yticks()\n",
    "    xticks(rotation=-90)\n",
    "    tight_layout()\n",
    "    \n",
    "def plot_graph(metric, method, nc, df):\n",
    "    G = nx.Graph()\n",
    "    d = sch.distance.pdist(df.values, metric=metric)\n",
    "    X = sch.distance.squareform(d)+1\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            G.add_edge(i,j,weight=1/X[i,j]) # weight=X[i,j]\n",
    "    Z = sch.linkage(d, method=method, optimal_ordering=True)\n",
    "    p_clusters= sch.fcluster(Z,t=nc,criterion='maxclust')\n",
    "    draw_communities(G,p_clusters, df.index)\n",
    "    tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance counters error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_error = Analyser(\"databases/fingerprints/hpc_belgica/v3/3mm_LARGE_DATASET_mem.dat\")\n",
    "\n",
    "def test(self,verbose=False):\n",
    "    # find the moda shape\n",
    "    count_shapes= defaultdict(lambda:0)\n",
    "    for r in self.data['data']:\n",
    "        count_shapes[np.shape(r)]+=1\n",
    "    moda_shape= max(count_shapes,key=count_shapes.get)\n",
    "    data_moda= [d for d in self.data['data'] if np.shape(d) == moda_shape]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Moda shape counts {:.2f}%\".format(count_shapes[moda_shape]/len(count_shapes.values())*100))\n",
    "        print(count_shapes[moda_shape], sum(count_shapes.values()))\n",
    "\n",
    "    el= int(count_shapes[moda_shape]*0.3)//2\n",
    "    data_moda= np.asarray(data_moda)\n",
    "    med_avg= np.sort(data_moda,axis=0)\n",
    "\n",
    "    if el != 0:\n",
    "        med_avg= med_avg[el:-el]\n",
    "\n",
    "    def diff(x):\n",
    "        x= np.concatenate( (x[:,0:1,:] , x[:,1:,:]-x[:,:-1,:]), axis=1 )/self.data['sample_period']\n",
    "        return x\n",
    "\n",
    "    med_avg= diff(med_avg)\n",
    "    std_avg= med_avg.std(axis=0)\n",
    "    med_avg= med_avg.mean(axis=0)\n",
    "\n",
    "    # create the dataframe\n",
    "    med_avg= pd.DataFrame(med_avg, columns=flat(self.data['to_monitor']))\n",
    "    std_avg= pd.DataFrame(std_avg, columns=flat(self.data['to_monitor']))\n",
    "\n",
    "    # quality of the samples (experimental)\n",
    "    if verbose:\n",
    "        q= std_avg.values/med_avg.values\n",
    "        print(\"AVG 68% samples error\", np.nanmean(q)*100)\n",
    "        print(\"AVG 99% samples error\", np.nanmean(3*q)*100)\n",
    "\n",
    "        print(\"MAX 68% samples error\", np.nanmax(q)*100)\n",
    "        print(\"MAX 99% samples error\", np.nanmax(3*q)*100)\n",
    "\n",
    "    return med_avg, std_avg\n",
    "\n",
    "\n",
    "for d, dim in enumerate(flat(prog_error.data[\"to_monitor\"])):\n",
    "    fname = flat(prog_error.data[\"to_monitor\"])[d]\n",
    "    print(fname)\n",
    "    matplotlib_rc_2()\n",
    "    \n",
    "    count_shapes= defaultdict(lambda:0)\n",
    "    for r in prog_error.data['data']:\n",
    "        count_shapes[np.shape(r)]+=1\n",
    "    moda_shape= max(count_shapes,key=count_shapes.get)\n",
    "    data_moda= [d for d in prog_error.data['data'] if np.shape(d) == moda_shape]\n",
    "    for e in data_moda:\n",
    "        aux= np.array(e)\n",
    "        aux= np.concatenate( (aux[0:1,:] , aux[1:,:]-aux[:-1,:]), axis=0 )/prog_error.data['sample_period']\n",
    "        plot(aux[:,d])\n",
    "    \n",
    "#     for e in prog_error.data['data']:\n",
    "#         aux= np.array(e)\n",
    "#         aux= np.concatenate( (aux[0:1,:] , aux[1:,:]-aux[:-1,:]), axis=0 )/prog_error.data['sample_period']\n",
    "#         plot(aux[:,d])\n",
    "    \n",
    "    med_avg, std_avg= test(prog_error,0)\n",
    "    aux= med_avg[dim].values\n",
    "    aux_std= std_avg[dim].values\n",
    "    \n",
    "    \n",
    "    fill_between(np.arange(aux.shape[0]), aux-1*aux_std, aux+1*aux_std,color='k',zorder=1)\n",
    "    plot(aux,c='r',linewidth=2)\n",
    "    ylabel(fname)\n",
    "    xlabel(\"Sample\")\n",
    "    savefig(f\"fingerprint/error_3mm_{fname}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterization by input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_all = pd.read_csv(\"databases/inputs_all.csv\",index_col=0).T\n",
    "d = sch.distance.pdist(df_input_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "\n",
    "labels = sch.fcluster(Z,t=5,criterion='maxclust')\n",
    "pdic = defaultdict(list)\n",
    "for l, p in zip(labels, df_input_all.index):\n",
    "    pdic[l].append(p.split('_')[0])\n",
    "for k in pdic:\n",
    "    count = Counter(pdic[k])\n",
    "    count = {k:count[k] for k in count if count[k] > 1 }\n",
    "    pdic[k] = list(count.keys())\n",
    "\n",
    "draw_clusters(pdic)\n",
    "plot_dendogram(Z, list(df_input_all.index.str.replace('_DATASET_mem.dat','')))\n",
    "savefig('fingerprint/dendograma_input_size.pdf')\n",
    "plot_graph('canberra', 'ward', 5, df_input_all)\n",
    "savefig('fingerprint/graph_input_size.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_all = pd.read_csv(\"databases/inputs_all.csv\",index_col=0).T\n",
    "d = sch.distance.pdist(df_input_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "\n",
    "labels = sch.fcluster(Z,t=5,criterion='maxclust')\n",
    "pdic = defaultdict(list)\n",
    "for l, p in zip(labels, df_input_all.index):\n",
    "    pdic[l].append(p)\n",
    "\n",
    "aux = df_input_all.copy()\n",
    "aux.index = aux.index.str.replace(\"_DATASET_mem.dat\",\"\")\n",
    "matplotlib_rc_2()\n",
    "for c, (k, p) in enumerate(pdic.items()):\n",
    "    x = [x.replace(\"_DATASET_mem.dat\",\"\") for x in p]\n",
    "    aux.loc[x].T.plot()\n",
    "    legend(loc=\"upper left\",ncol=3,fontsize=10)\n",
    "    ylabel(\"Input size\")\n",
    "    xlabel(\"Normalized time\")\n",
    "    tight_layout()\n",
    "    savefig(\"fingerprint/cluster_input_{}\".format(c))\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation before after clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_input_all = pd.read_csv(\"databases/inputs_all.csv\",index_col=0).T\n",
    "d = sch.distance.pdist(df_input_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "labels = sch.fcluster(Z,t=5,criterion='maxclust')\n",
    "\n",
    "# d= sch.distance.pdist(df.values, metric=best['metric'])\n",
    "# X= sch.distance.squareform(d)+1\n",
    "# df_aux= pd.DataFrame(X,columns=df.index)\n",
    "\n",
    "df_aux = df_input_all.T\n",
    "columns = [df_aux.columns.tolist()[i] for i in list((np.argsort(labels)))]\n",
    "df_new = df_aux.reindex(columns, axis=1)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (24,24)\n",
    "plot_corr(df_aux)\n",
    "savefig(\"fingerprint/corr_before_input_size.pdf\")\n",
    "plot_corr(df_new)\n",
    "savefig(\"fingerprint/corr_after_input_size.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterization by floating point operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.style.use('grayscale')\n",
    "df_fp_all = pd.read_csv(\"databases/fp_all.csv\",index_col=0)\n",
    "df_norm_fp_all = (df_fp_all - df_fp_all.mean()) / (df_fp_all.max() - df_fp_all.min())\n",
    "df_norm_fp_all = df_norm_fp_all.T\n",
    "\n",
    "d = sch.distance.pdist(df_norm_fp_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "labels = sch.fcluster(Z, t=8,criterion='maxclust')\n",
    "\n",
    "pdic = defaultdict(list)\n",
    "for l, p in zip(labels, df_norm_fp_all.index):\n",
    "    pdic[l].append(p.split('_')[0])\n",
    "\n",
    "draw_clusters(pdic)\n",
    "plot_dendogram(Z, list(df_norm_fp_all.index.str.replace('_DATASET_mem.dat','')))\n",
    "savefig('fingerprint/dendograma_floating.pdf')\n",
    "plot_graph('canberra', 'ward', 8, df_fp_all.T)\n",
    "savefig('fingerprint/graph_floating.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fp_all = pd.read_csv(\"databases/fp_all.csv\",index_col=0)\n",
    "df_norm_fp_all = (df_fp_all - df_fp_all.mean()) / (df_fp_all.max() - df_fp_all.min())\n",
    "df_norm_fp_all = df_norm_fp_all.T\n",
    "\n",
    "d = sch.distance.pdist(df_norm_fp_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "labels = sch.fcluster(Z, t=8,criterion='maxclust')\n",
    "\n",
    "pdic = defaultdict(list)\n",
    "for l, p in zip(labels, df_norm_fp_all.index):\n",
    "    pdic[l].append(p)\n",
    "\n",
    "aux = df_norm_fp_all\n",
    "aux.index= aux.index.str.replace('_DATASET_mem.dat','')\n",
    "aux = aux.T\n",
    "matplotlib_rc_1()\n",
    "for c, (k, p) in enumerate(pdic.items()):\n",
    "    x = [x.replace('_DATASET_mem.dat','') for x in p]\n",
    "    aux[x].plot(figsize=(10,10))\n",
    "    legend(loc='upper left',ncol=3,fontsize=10)\n",
    "    ylabel('Floating point operations')\n",
    "    xlabel('Normalized time')\n",
    "    tight_layout()\n",
    "    savefig('fingerprint/cluster_fp_{}'.format(c))\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation before after clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fp_all = pd.read_csv(\"databases/fp_all.csv\",index_col=0)\n",
    "df_norm_fp_all = (df_fp_all - df_fp_all.mean()) / (df_fp_all.max() - df_fp_all.min())\n",
    "df_norm_fp_all = df_norm_fp_all.T\n",
    "\n",
    "d = sch.distance.pdist(df_norm_fp_all.values, 'canberra')\n",
    "Z = sch.linkage(d, 'ward', optimal_ordering=True)\n",
    "labels = sch.fcluster(Z, t=8,criterion='maxclust')\n",
    "\n",
    "# d= sch.distance.pdist(df.T.values, metric=best['metric'])\n",
    "# X= sch.distance.squareform(d)+1\n",
    "# df_aux = pd.DataFrame(X,columns=df.T.index)\n",
    "\n",
    "df_aux = df_fp_all\n",
    "columns = [df_aux.columns.tolist()[i] for i in list((np.argsort(labels)))]\n",
    "df_new = df_aux.reindex(columns, axis=1)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (24,24)\n",
    "plot_corr(df_aux)\n",
    "savefig(\"fingerprint/corr_before_float.pdf\")\n",
    "plot_corr(df_new)\n",
    "savefig(\"fingerprint/corr_after_float.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprint workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = []\n",
    "p2 = []\n",
    "p3 = []\n",
    "p4 = []\n",
    "px = Analyser('databases/fingerprints/hpc_belgica/v3/2mm_EXTRALARGE_DATASET_mem.dat')\n",
    "for r in px.data[\"data\"]:\n",
    "    aux = np.asarray(r)\n",
    "    p1.append(aux[:,0]) # PERF_COUNT_HW_INSTRUCTIONS\n",
    "p2 = px.df['PERF_COUNT_HW_INSTRUCTIONS'].values\n",
    "_, y0= px.interpolate(feature='PERF_COUNT_HW_INSTRUCTIONS', npoints= 100, filter_signal= False) #'input_size'\n",
    "p3 = y0\n",
    "_, y0= px.interpolate(feature='PERF_COUNT_HW_INSTRUCTIONS', npoints= 100) #'input_size'\n",
    "p4 = y0\n",
    "matplotlib_rc_2()\n",
    "for x in p1:\n",
    "    plot(np.diff(x))\n",
    "ylabel(\"Instructions executed\")\n",
    "xlabel(\"Sample\")\n",
    "tight_layout()\n",
    "savefig(\"fingerprint/workflow.pdf\")\n",
    "figure()\n",
    "plot(p2)\n",
    "ylabel(\"Instructions executed\")\n",
    "xlabel(\"Sample\")\n",
    "tight_layout()\n",
    "savefig(\"fingerprint/workflow_1.pdf\")\n",
    "figure()\n",
    "plot(p3)\n",
    "ylabel(\"Instructions executed\")\n",
    "xlabel(\"Sample\")\n",
    "tight_layout()\n",
    "savefig(\"fingerprint/workflow_2.pdf\")\n",
    "figure()\n",
    "plot(p4)\n",
    "ylabel(\"Instructions executed\")\n",
    "xlabel(\"Sample\")\n",
    "tight_layout()\n",
    "savefig(\"fingerprint/workflow_3.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical points with fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"databases/inputs_all.csv\",index_col=0)\n",
    "for p in df.columns:\n",
    "    print(p)\n",
    "    matplotlib_rc_2()\n",
    "    df[p].plot(c='g')\n",
    "    \n",
    "    dif= np.diff(df[p])\n",
    "    dif= np.diff(dif)\n",
    "    dif= abs(dif)\n",
    "    idx= np.where(dif>dif.mean())[0]+1\n",
    "    X = numpy.hstack((idx,df[p].values[idx])).reshape((-1,2),order='F')\n",
    "    km= KMeans(n_clusters=7, random_state=0).fit(X)\n",
    "    centers= km.cluster_centers_\n",
    "    centers= np.vstack( (centers, [0, df[p].values[0]]) )\n",
    "    centers= np.vstack( (centers, [99, df[p].values[99]]) )\n",
    "    centers= centers[centers[:,0].argsort()]\n",
    "    \n",
    "    plot(centers[:,0], centers[:,1],c='b')\n",
    "    scatter(centers[:,0], centers[:,1],c='b')\n",
    "    xlabel(\"Normalized time\")\n",
    "    ylabel(\"Input size\")\n",
    "    tight_layout()\n",
    "    savefig(f\"phases/critical_points/fp_phase_{p.replace('.dat','.pdf')}\")\n",
    "    legend()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Load the data and prepare the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    \"\"\"\n",
    "    This is just to plot the names\n",
    "    \"\"\"\n",
    "    app_titles = {\n",
    "        \"black\": \"Blackscholes\", \"body\": \"Bodytrack\", \"canneal\": \"Canneal\",\n",
    "        \"dedup\": \"Dedup\", \"fluid\": \"Fluianimate\", \"freq\": \"Freqmine\",\n",
    "        \"openmc\": \"Openmc\", \"rtview\": \"Raytrace\", \"swap\": \"Swaptions\",\n",
    "        \"vips\": \"Vips\", \"xhpl\": \"HPL\", \"x264\": \"x264\",\n",
    "        \"ferret\": \"Ferret\",\n",
    "    }\n",
    "    for k in app_titles:\n",
    "        if k in name:\n",
    "            return app_titles[k]\n",
    "        \n",
    "def map_color(col):\n",
    "    clist= (col-min(col))/(max(col)-min(col))*255\n",
    "    colors= map(lambda x: cm.inferno.colors[int(x)%256], clist)\n",
    "    cmp= (col.unique()-min(col))/(max(col)-min(col))*255\n",
    "    cmp= map(lambda x: cm.inferno.colors[int(x)%256], cmp)\n",
    "    cmap= dict(zip(col.unique(),cmp))\n",
    "\n",
    "    patchs= []\n",
    "    for l,c in cmap.items():\n",
    "        patchs.append(mpatches.Patch(color=c, label=l))\n",
    "\n",
    "    return list(colors), patchs\n",
    "\n",
    "def create_df_resum():\n",
    "    global df_resum\n",
    "    if not (df_resum is None):\n",
    "        return df_resum\n",
    "\n",
    "    run_col = energydb[\"run\"]\n",
    "    cursor = run_col.aggregate([\n",
    "        {\n",
    "            \"$match\": { \"config.data_descriptor.extras.sensors\": {'values': ['info', 'time']} },\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"keys\": {\"$push\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ],  allowDiskUse=True)\n",
    "    df_resum= []\n",
    "    for d in tqdm(cursor):\n",
    "        for k in d[\"keys\"]:\n",
    "            if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "                data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "                data[\"_id\"]= str(data[\"_id\"])\n",
    "                json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "            data = PascalData(f\"cache/{k}.json\")\n",
    "            df= data.energy()\n",
    "            df[\"nsamples\"]= data.dataframe_group(\"sensors\")[\"info\"].apply(len)\n",
    "            df[\"name\"]= data.config[\"pkg\"]\n",
    "            df_resum.append(df)\n",
    "    df_resum = pd.concat(df_resum)\n",
    "    return df_resum\n",
    "\n",
    "def create_df_times():\n",
    "    global df_times\n",
    "    if not (df_times is None):\n",
    "        return df_times\n",
    "    run_col = energydb[\"run\"]\n",
    "    cursor = run_col.aggregate([\n",
    "        {\n",
    "            \"$match\": { \"config.data_descriptor.keys\": [\"cores\", \"frequency\", \"input\", \"repetitions\"] },\n",
    "        },\n",
    "        {\n",
    "            \"$match\": { \"config.data_descriptor.extras.sensors\": {'values': ['info', 'time']} },  \n",
    "        },\n",
    "        {\n",
    "            \"$group\":{\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"keys\": {\"$push\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True)\n",
    "    df_times= []\n",
    "    prog_names = defaultdict(lambda : 0)\n",
    "    for d in tqdm(cursor):\n",
    "        for k in d[\"keys\"]:\n",
    "            if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "                data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "                data[\"_id\"]= str(data[\"_id\"])\n",
    "                json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "            data = PascalData(f\"cache/{k}.json\")\n",
    "            df = data.energy()\n",
    "            if not (len(df[\"cores\"].unique()) > 1 and \\\n",
    "                len(df[\"frequency\"].unique()) > 1 and \\\n",
    "                len(df[\"input\"].unique()) > 1):\n",
    "                    continue\n",
    "\n",
    "            def filtering(df):\n",
    "                last_ins = np.sort(df[\"input\"].unique())[-5:]\n",
    "                df = df[df[\"input\"].isin(last_ins)]\n",
    "                df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "                df = df[df[\"frequency\"] < 2.3]\n",
    "                df[\"ipmi_energy\"] /= 1e3\n",
    "                df[\"input\"] = df[\"input\"].astype(int)\n",
    "                df[\"input\"] = df[\"input\"]-df[\"input\"].min()+1\n",
    "                return df\n",
    "            df = filtering(df)\n",
    "            df[\"name\"] = data.config[\"pkg\"]\n",
    "            prog_names[data.config[\"pkg\"]] += 1\n",
    "            if prog_names[data.config[\"pkg\"]] > 1:\n",
    "                df[\"name\"] +=  \"_\" + str(prog_names[data.config[\"pkg\"]]-1)\n",
    "            df_times.append(df)\n",
    "\n",
    "    df_times = pd.concat(df_times)\n",
    "    df_times[[\"input\",\"cores\",\"frequency\"]] = df_times[[\"input\",\"cores\",\"frequency\"]].astype(float)\n",
    "    df_times = df_times.sort_values([\"input\",\"cores\",\"frequency\"])\n",
    "    return df_times\n",
    "\n",
    "def create_df_models():\n",
    "    \"\"\"\n",
    "    Grab the results group by application arguments\n",
    "    \"\"\"\n",
    "    global df_models\n",
    "    if not (df_models is None):\n",
    "        return df_models\n",
    "    model_col = energydb[\"model\"]\n",
    "    models = model_col.aggregate([\n",
    "            {\n",
    "                \"$group\":\n",
    "                {\n",
    "                    \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                    #\"all\": {\"$push\": \"$$ROOT\" },\n",
    "                    \"pkgs\": {\"$push\": \"$config.pkg\" },\n",
    "                    \"keys\": {\"$push\": \"$_id\" },\n",
    "                    \"maes\": {\"$push\": \"$info.mae\" },\n",
    "                    \"tss\": {\"$push\": \"$train_sz\" },\n",
    "                    \"idxs\": {\"$push\": \"$train_idx\" },\n",
    "                    \"types\": {\"$push\": \"$info.type\"},\n",
    "                    \"split_types\": {\"$push\": \"$split_type\" },\n",
    "                    \"nitem\": {\"$sum\": 1}\n",
    "                }\n",
    "            }\n",
    "        ], \n",
    "        allowDiskUse=True)\n",
    "    df_models= []\n",
    "    for app in tqdm(models):\n",
    "        ens = []\n",
    "        refs= []\n",
    "        for kk, ii in zip(app[\"keys\"], app[\"idxs\"]):\n",
    "            if not os.path.isfile(f\"cache/{kk}.json\"):\n",
    "                data = model_col.find_one( {\"_id\": ObjectId(kk) } )\n",
    "                data[\"_id\"]= str(data[\"_id\"])\n",
    "                json.dump(data, open(f\"cache/{kk}.json\", \"w+\"))\n",
    "            data = PascalModel(f\"cache/{kk}.json\")\n",
    "            ens.append(data.data.loc[ii][\"ipmi_energy\"].sum())\n",
    "            refs.append(data)\n",
    "\n",
    "        X = np.transpose([app[\"pkgs\"], app[\"tss\"], app[\"types\"], app[\"split_types\"], app[\"maes\"], ens, refs])\n",
    "        df = pd.DataFrame(X, columns=[\"name\",\"ts\", \"type\", \"split_type\", \"mpe\", \"energy\", \"ref\"])\n",
    "        df[[\"ts\", \"mpe\", \"energy\"]] = df[[\"ts\", \"mpe\", \"energy\"]].astype(float)\n",
    "\n",
    "        df_models.append(df)\n",
    "\n",
    "    df_models = pd.concat(df_models)\n",
    "    df_models[\"title\"] = df_models[\"name\"].apply(get_title)\n",
    "    return df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_resum = create_df_resum()\n",
    "display(df_resum)\n",
    "print(\"Total number of executions :\", df_resum.shape[0])\n",
    "print(\"Total number of samples :\", df_resum.nsamples.sum())\n",
    "print(\"Total energy spent (Mj) :\", df_resum.ipmi_energy.sum()/1e6)\n",
    "print(\"Total time spent (Days):\", df_resum.total_time.sum()/(60*60*24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_times = create_df_times()\n",
    "display(df_times)\n",
    "print(\"Total from complete executions\")\n",
    "print(\"Total number of executions : \", df_times.shape[0])\n",
    "print(\"Total energy spent (Mj) : \", df_times.ipmi_energy.sum()/1e3)\n",
    "print(\"Total time spent (Days) : \", df_times.total_time.sum()/(60*60*24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_times[(df_times[\"cores\"]==32)&(df_times[\"input\"]==5)].groupby(\"name\").ipmi_energy.std()/df_times[(df_times[\"cores\"]==32)&(df_times[\"input\"]==5)].groupby(\"name\").ipmi_energy.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = create_df_times()\n",
    "matplotlib_rc_2()\n",
    "\n",
    "def rel_max_min(df,col):\n",
    "    min_f = df[df[\"frequency\"]==df[\"frequency\"].min()][col].values[0]\n",
    "    max_f = df[df[\"frequency\"]==df[\"frequency\"].max()][col].values[0]\n",
    "    return min_f/max_f\n",
    "\n",
    "df_rel_min_max_freq = df_times.groupby([\"name\",\"input\",\"cores\"]).apply(\n",
    "    partial(rel_max_min, col=\"total_time\")).reset_index()\n",
    "df_rel_min_max_en = df_times.groupby([\"name\",\"input\",\"cores\"]).apply(\n",
    "    partial(rel_max_min, col=\"ipmi_energy\")).reset_index()\n",
    "\n",
    "hist(df_rel_min_max_freq[0], label=\"Time min freq/Time max freq\",bins=np.arange(0,5,0.1),alpha=0.5)\n",
    "hist(df_rel_min_max_en[0], label=\"Energy min freq/Energy max freq\",bins=np.arange(0,5,0.1),alpha=0.5)\n",
    "tight_layout()\n",
    "legend()\n",
    "savefig(\"models/min_max_freq_cmp.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_times = create_df_times()\n",
    "for app in df_times.name.unique():\n",
    "    dfx = df_times[df_times[\"name\"]==app]\n",
    "    dfx[\"edp\"] = dfx[\"ipmi_energy\"]*dfx[\"total_time\"]\n",
    "    dfx = dfx[dfx[\"input\"]==dfx[\"input\"].max()]\n",
    "    \n",
    "    print(app)\n",
    "    matplotlib_rc_1()\n",
    "    fig, axs = subplots(2, 2)\n",
    "    colors, labels= map_color(dfx[\"cores\"])\n",
    "    dfx.plot.scatter(x=\"frequency\", y=\"edp\", c=colors, ax= axs[0][0])\n",
    "    dfx.plot.scatter(x=\"total_time\", y=\"ipmi_energy\", c=colors, ax= axs[0][1])\n",
    "    dfx.plot.scatter(x=\"frequency\", y=\"total_time\", c=colors, ax= axs[1][0])\n",
    "    dfx.plot.scatter(x=\"frequency\", y=\"ipmi_energy\", c=colors, ax= axs[1][1])\n",
    "    subplots_adjust(left=0.07, right=0.93, wspace=0.25, hspace=0.35)\n",
    "    axs[0][0].set_xlabel(\"Frequency (GHz)\")\n",
    "    axs[0][0].set_ylabel(\"EDP\")\n",
    "    axs[0][1].set_xlabel(\"Time (s)\")\n",
    "    axs[0][1].set_ylabel(\"Energy (J)\")\n",
    "    axs[1][0].set_xlabel(\"Frequency (GHz)\")\n",
    "    axs[1][0].set_ylabel(\"Time (s)\")\n",
    "    axs[1][1].set_xlabel(\"Frequency (GHz)\")\n",
    "    axs[1][1].set_ylabel(\"Energy (J)\")\n",
    "    legend(handles=labels, loc=\"upper right\", bbox_to_anchor=(1.2, 2.35))\n",
    "    #tight_layout()\n",
    "    savefig(f\"models/metrics/{app}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = create_df_times()\n",
    "matplotlib_rc_2()\n",
    "colors, labels= map_color(df_times[\"cores\"])\n",
    "df_times.plot.scatter(x=\"total_time\", y=\"ipmi_energy\",c=colors)\n",
    "legend(handles=labels,fontsize=10)\n",
    "tight_layout()\n",
    "savefig(\"models/metrics/all.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = create_df_times()\n",
    "def speedup(df):\n",
    "    if df[df[\"cores\"]==1].total_time.shape[0]>0:\n",
    "        #print(df)\n",
    "        #print(df.total_time.values/df[df[\"cores\"]==1].total_time.values)\n",
    "        df.total_time= df[df[\"cores\"]==1].total_time.iloc[0]/df.total_time\n",
    "    else:\n",
    "        df.total_time= None\n",
    "    df= df.rename(columns={\"total_time\":\"speedup\"})\n",
    "    return df[[\"cores\",\"speedup\",\"ipmi_energy\"]].reset_index(drop=True)\n",
    "\n",
    "speedup = df_times.groupby([\"name\",\"frequency\",\"input\"]).apply(speedup)\n",
    "speedup = speedup.reset_index().dropna().drop(columns=[\"level_3\"])\n",
    "speedup[\"parallel_frac\"]= (speedup[\"cores\"]*speedup[\"speedup\"]-speedup[\"cores\"])/( (speedup[\"cores\"]-1)*speedup[\"speedup\"] )\n",
    "display(speedup)\n",
    "\n",
    "ws = (speedup[\"cores\"]*speedup[\"speedup\"]-speedup[\"cores\"])/( (speedup[\"cores\"]-1)*speedup[\"speedup\"] )\n",
    "ws = ws.dropna()\n",
    "matplotlib_rc_2()\n",
    "ws[ (ws>=0)&(ws<=1) ].plot.hist()\n",
    "xlabel(\"Parallel fraction\")\n",
    "tight_layout()\n",
    "savefig(\"models/speedups.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D plots energy (model vs measured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Cores X Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "min_mpe = df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2 = df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")\n",
    "\n",
    "matplotlib_rc_2()\n",
    "mpes = []\n",
    "for idx, eq in aux2.iterrows():\n",
    "    model = eq.ref\n",
    "    #print(eq)\n",
    "\n",
    "    i = model.data[\"input\"].unique().max()\n",
    "    f = model.data[\"frequency\"].unique().max()\n",
    "    crs = sort(model.data[\"cores\"].unique())\n",
    "\n",
    "    X = np.array(list(prod_itertools([i],crs,[f])))\n",
    "    X = np.sort(X,axis=0)\n",
    "\n",
    "    en_pred = model.predict(X)\n",
    "    en_real = model.data[(model.data[\"frequency\"]==f)&\n",
    "                         (model.data[\"input\"]==i)].sort_values(\"cores\").ipmi_energy.values\n",
    "    \n",
    "    #title(eq[\"name\"].split(\"_\")[1].capitalize())\n",
    "    mpe = sum(abs(en_real-en_pred)/en_real)/len(en_real)*100\n",
    "    print(eq[\"name\"])\n",
    "    print(\"MSE\", sum((en_real-en_pred)**2)/len(en_real))\n",
    "    print(\"MPE\", mpe)\n",
    "    name = model.config[\"pkg\"]\n",
    "    mpes.append([name,mpe])\n",
    "    plot(crs, en_pred, label=\"model\")\n",
    "    scatter(crs, en_real, color=\"black\",label=\"measured values\")\n",
    "    xlabel(\"Cores\")\n",
    "    ylabel(\"Energy (kJ)\")\n",
    "    legend()\n",
    "    savefig(f\"models/energy/power/{name}.pdf\")\n",
    "    show()\n",
    "mpes = pd.DataFrame(mpes, columns=[\"name\",\"mpe\"])\n",
    "mpesc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Cores X Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/vitor/Documents/energy_scripts_superpc/create_db\"\n",
    "reload(plotdata)\n",
    "df_models = create_df_models()\n",
    "\n",
    "def plot3d(pascalmodel, outputname):\n",
    "    df = pascalmodel.data\n",
    "    df[pascalmodel.inputs] = df[pascalmodel.inputs].astype(float)\n",
    "    df_train= df.loc[pascalmodel.train_idx]\n",
    "\n",
    "    freqs = np.arange(1.1, 2.4, 0.1,dtype=float)\n",
    "    cores = np.arange(1, 32, 1, dtype=float)\n",
    "    plotdata.new_figure()\n",
    "    matplotlib_rc_2()\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.rcParams[\"legend.fontsize\"] = 14\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "    plt.rcParams[\"axes.labelsize\"] = 14\n",
    "\n",
    "    def update():\n",
    "        X = np.array(np.meshgrid(*[freqs, cores])).T.reshape(-1, 2)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        plotdata.setProps(xlabel='Frequencies (GHz)',\n",
    "                          ylabel='Active threads',\n",
    "                          zlabel='Power (W)',\n",
    "                          fontsize=18)\n",
    "\n",
    "        dfaux = df_train\n",
    "        dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_power\"],\n",
    "                        antialiased=True, color=\"red\",s=500,marker=\"*\")\n",
    "\n",
    "        dfaux = df.loc[set(df.index)-set(pascalmodel.train_idx)]\n",
    "        dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_power\"],\n",
    "                        antialiased=True, color=\"black\",s=100)\n",
    "\n",
    "        plotdata.plot3D(freqs, cores, Y, points=False, color_=\"b\")\n",
    "        plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Model\"])\n",
    "        #plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Min energy\",\"Model\"])\n",
    "        \n",
    "        \n",
    "        X = dfaux[[\"frequency\",\"cores\"]].values\n",
    "        Y = pascalmodel.predict(X)\n",
    "        mpe = sum(abs(dfaux[\"ipmi_power\"].values-Y))/len(Y)\n",
    "        name = pascalmodel.config[\"pkg\"]\n",
    "        mpes.append([name, mpe])\n",
    "        print(name, mpe)\n",
    "\n",
    "    plotdata.update_user = update\n",
    "    update()\n",
    "    plotdata.ax.view_init(40, -60)\n",
    "    tight_layout()\n",
    "    \n",
    "    savefig(outputname)\n",
    "    show()\n",
    "\n",
    "aux = df_models[df_models[\"ref\"].apply(lambda x : (x.data[\"cores\"].unique().shape[0]>2)\n",
    "                                                   &(x.data[\"frequency\"].unique().shape[0]>2) )]\n",
    "min_mpe = aux[(aux[\"ts\"]==10)&(aux[\"type\"]==\"pw_equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2 = aux[aux.mpe.isin(min_mpe)].drop_duplicates(\"title\")\n",
    "mpes = []\n",
    "for idx,pascalmodel in aux2.iterrows():\n",
    "    model = pascalmodel.ref\n",
    "    name = pascalmodel[\"name\"]\n",
    "    plot3d(model, outputname=f\"models/power/{name}.pdf\")\n",
    "\n",
    "mpes = pd.DataFrame(mpes, columns=[\"name\",\"mpe\"])\n",
    "print(mpes.mean())\n",
    "mpes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D plots energy (model vs measured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Cores X Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(plotdata)\n",
    "df_models = create_df_models()\n",
    "\n",
    "def plot3d(pascalmodel, outputname):\n",
    "    df = pascalmodel.data\n",
    "    df[pascalmodel.inputs] = df[pascalmodel.inputs].astype(float)\n",
    "    df_train= df.loc[pascalmodel.train_idx]\n",
    "\n",
    "    freqs = np.arange(1.1, 2.4, 0.1)\n",
    "    cores = np.arange(1, 32, 1)\n",
    "    \n",
    "    matplotlib_rc_2()\n",
    "    plotdata.new_figure()\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.rcParams[\"legend.fontsize\"] = 14\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "    plt.rcParams[\"axes.labelsize\"] = 14\n",
    "\n",
    "    def update(val):\n",
    "        X = np.array(np.meshgrid(\n",
    "            [int(val)], cores, freqs)).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        plotdata.setProps(xlabel='Frequencies (GHz)',\n",
    "                          ylabel='Active threads',\n",
    "                          zlabel='Energy (KJ)',\n",
    "                          fontsize=18)\n",
    "\n",
    "        dfaux = df_train[df_train[\"input\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"red\",s=500,marker=\"*\")\n",
    "\n",
    "        dfaux = df.loc[set(df.index)-set(pascalmodel.train_idx)]\n",
    "        dfaux = dfaux[dfaux[\"input\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"black\",s=100)\n",
    "        \n",
    "#         if len(best) > 2:\n",
    "#             plotdata.ax.scatter([best[2]],\n",
    "#                             [best[1]],\n",
    "#                             [best[3]],\n",
    "#                             antialiased=True, color=\"y\",s=100)\n",
    "        \n",
    "        plotdata.plot3D(freqs, cores, Y, points=False, color_=\"b\")\n",
    "        plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Model\"])\n",
    "        #plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Min energy\",\"Model\"])\n",
    "\n",
    "    best= []\n",
    "    for i, arg in enumerate(pascalmodel.config[\"arguments\"]):\n",
    "        X = np.array(np.meshgrid(\n",
    "            [float(i)], cores, freqs)).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        idx = np.argmin(Y)\n",
    "        if i == 2:\n",
    "            best= list(X[idx])+[Y[idx]]\n",
    "\n",
    "    plotdata.update_user = update\n",
    "    update(3)\n",
    "    plotdata.ax.view_init(30, 60)\n",
    "    tight_layout()\n",
    "    savefig(outputname)\n",
    "    show()\n",
    "\n",
    "min_mpe = df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2 = df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")\n",
    "for row in aux2.values:\n",
    "    model= row[6]\n",
    "    print(row[0])\n",
    "    plot3d(model,outputname=f\"models/energy/freq_cores/{row[0]}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Input X Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "def plot3d(pascalmodel, outputname):\n",
    "    df = pascalmodel.data\n",
    "    df[pascalmodel.inputs] = df[pascalmodel.inputs].astype(float)\n",
    "    df_train= df.loc[pascalmodel.train_idx]\n",
    "    # print(pascalmodel.config)\n",
    "\n",
    "    freqs = np.arange(1.1, 2.4, 0.1)\n",
    "    cores = np.arange(1, 32, 1)\n",
    "    inps = np.arange(1, 6, 1)\n",
    "    matplotlib_rc_2()\n",
    "    plotdata.new_figure()\n",
    "    \n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.rcParams[\"legend.fontsize\"] = 14\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "    plt.rcParams[\"axes.labelsize\"] = 14\n",
    "\n",
    "    def update(val):\n",
    "        X = np.array(np.meshgrid(\n",
    "            inps, [int(val)], freqs)).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        plotdata.setProps(xlabel='Frequency (GHz)',\n",
    "                          ylabel='Input size',\n",
    "                          zlabel='Energy (KJ)',\n",
    "                          fontsize=18)\n",
    "\n",
    "        dfaux = df_train[df_train[\"cores\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"frequency\", \"input\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"input\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"red\",s=500,marker=\"*\")\n",
    "\n",
    "        dfaux = df.loc[set(df.index)-set(pascalmodel.train_idx)]\n",
    "        dfaux = dfaux[dfaux[\"cores\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"frequency\",\"input\"])\n",
    "        plotdata.ax.scatter(dfaux[\"frequency\"],\n",
    "                        dfaux[\"input\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"black\",s=100)\n",
    "        \n",
    "#         if len(best) > 2:\n",
    "#             plotdata.ax.scatter([best[2]],\n",
    "#                             [best[1]],\n",
    "#                             [best[3]],\n",
    "#                             antialiased=True, color=\"y\",s=100)\n",
    "        \n",
    "        plotdata.plot3D(freqs, inps, Y, points=False, color_=\"b\")\n",
    "        plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Model\"])\n",
    "        plotdata.ax.set_xlim(min(freqs), max(freqs))\n",
    "        plotdata.ax.set_ylim(min(inps), max(inps))\n",
    "        #plotdata.ax.legend([\"train values\",\"measured values\",\"min energy\",\"model\"],fontsize=20)\n",
    "\n",
    "    best= []\n",
    "    for i, arg in enumerate(pascalmodel.config[\"arguments\"]):\n",
    "        X = np.array(np.meshgrid(\n",
    "            [float(i)], cores, freqs)).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        idx = np.argmin(Y)\n",
    "        if i == 2:\n",
    "            best= list(X[idx])+[Y[idx]]\n",
    "\n",
    "    plotdata.update_user = update\n",
    "    update(32)\n",
    "    plotdata.ax.view_init(30, 240)\n",
    "    tight_layout()\n",
    "    savefig(outputname)\n",
    "    show()\n",
    "\n",
    "\n",
    "min_mpe= df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2= df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")\n",
    "for row in aux2.values:\n",
    "    model= row[6]\n",
    "    print(row[0])\n",
    "    plot3d(model,outputname=f\"models/energy/freq_inps/{row[0]}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Input X Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "def plot3d(pascalmodel, outputname):\n",
    "    df = pascalmodel.data\n",
    "    df[pascalmodel.inputs] = df[pascalmodel.inputs].astype(float)\n",
    "    df_train= df.loc[pascalmodel.train_idx]\n",
    "    # print(pascalmodel.config)\n",
    "\n",
    "    freqs = np.arange(1.1, 2.4, 0.1)\n",
    "    cores = np.arange(1, 32, 1)\n",
    "    inps = np.arange(1, 6, 1)\n",
    "    matplotlib_rc_2()\n",
    "    plotdata.new_figure()\n",
    "    \n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.rcParams[\"legend.fontsize\"] = 14\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "    plt.rcParams[\"axes.labelsize\"] = 14\n",
    "\n",
    "    def update(val):\n",
    "        X = np.array(np.meshgrid(\n",
    "            inps, cores, [float(val)])).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        plotdata.setProps(xlabel='Input size',\n",
    "                          ylabel='Active threads',\n",
    "                          zlabel='Energy (KJ)',\n",
    "                          fontsize=18)\n",
    "\n",
    "        dfaux = df_train[df_train[\"frequency\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"cores\", \"input\"])\n",
    "        plotdata.ax.scatter(dfaux[\"input\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"red\",s=500,marker=\"*\")\n",
    "\n",
    "        dfaux = df.loc[set(df.index)-set(pascalmodel.train_idx)]\n",
    "        dfaux = dfaux[dfaux[\"frequency\"] == val]\n",
    "        dfaux = dfaux.sort_values([\"cores\",\"input\"])\n",
    "        plotdata.ax.scatter(dfaux[\"input\"],\n",
    "                        dfaux[\"cores\"],\n",
    "                        dfaux[\"ipmi_energy\"],\n",
    "                        antialiased=True, color=\"black\",s=100)\n",
    "        \n",
    "#         if len(best) > 2:\n",
    "#             plotdata.ax.scatter([best[2]],\n",
    "#                             [best[1]],\n",
    "#                             [best[3]],\n",
    "#                             antialiased=True, color=\"y\",s=100)\n",
    "        \n",
    "        plotdata.plot3D(inps, cores, Y, points=False, color_=\"b\")\n",
    "        plotdata.ax.legend([\"Trained values\",\"Measured values\",\"Model\"])\n",
    "        #plotdata.ax.legend([\"train values\",\"measured values\",\"min energy\",\"model\"],fontsize=20)\n",
    "\n",
    "    best= []\n",
    "    for i, arg in enumerate(pascalmodel.config[\"arguments\"]):\n",
    "        X = np.array(np.meshgrid(\n",
    "            [float(i)], cores, freqs)).T.reshape(-1, 3)\n",
    "        Y = pascalmodel.predict(X)\n",
    "        idx = np.argmin(Y)\n",
    "        if i == 2:\n",
    "            best= list(X[idx])+[Y[idx]]\n",
    "\n",
    "    plotdata.update_user = update\n",
    "    update(max(df_train.frequency.unique()))\n",
    "    plotdata.ax.view_init(30, 60)\n",
    "    tight_layout()\n",
    "    savefig(outputname)\n",
    "    show()\n",
    "\n",
    "\n",
    "min_mpe= df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2= df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")\n",
    "for row in aux2.values:\n",
    "    model= row[6]\n",
    "    print(row[0])\n",
    "    plot3d(model,outputname=f\"models/energy/cores_inps/{row[0]}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ml_models/data03_random.csv\",index_col=0)\n",
    "files = [\"completo_fluid_2.json\", \"completo_openmc_kernel_novo.json\",\n",
    "        \"completo_rtview_1.json\", \"completo_swaptions_1.json\", \n",
    "         \"completo_vips_4.json\", \"completo_x264_4.json\",\n",
    "        \"completo_xhpl.json\", \"completo_black_3.json\",\n",
    "        \"completo_bodytrack_3.json\", \"completo_canneal_3.json\",\n",
    "        \"completo_dedup_3.json\",\"completo_ferret_3.json\"]\n",
    "not_file = ['completo_freqmine_1.json', 'completo_freqmine_2.json','completo_fluid_1.json']\n",
    "df = df[df[\"name\"]!=\"SVR_gridsearch\"]\n",
    "df = df[df[\"file\"].isin(files)]\n",
    "df = df[df[\"ts\"]<100]\n",
    "#df = df[~df[\"file\"].isin(not_file)]\n",
    "df = df.groupby([\"name\",\"ts\"]).mean().reset_index()\n",
    "\n",
    "ax = None\n",
    "matplotlib_rc_2()\n",
    "for name in df[\"name\"].unique():\n",
    "    aux = df[df[\"name\"]==name]\n",
    "    ax = aux.plot(x = \"ts\", y = \"mse\", ax = ax, label = name, logy=True)\n",
    "    ylabel(\"Mean squared error\")\n",
    "    xlabel(\"Training data size\")\n",
    "\n",
    "savefig(\"experiments/ml_models.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall MPE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "df_overhad = []\n",
    "min_mpe = df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux1 = df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")[[\"title\",\"ts\",\"type\",\"mpe\"]]\n",
    "df_overhad.append(aux1)\n",
    "print(\"Table with best equation models using 10 points\")\n",
    "display(aux1)\n",
    "\n",
    "min_mpe = df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"svr\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2 = df_models[df_models.mpe.isin(min_mpe)].drop_duplicates(\"title\")[[\"title\",\"ts\",\"type\",\"mpe\"]]\n",
    "df_overhad.append(aux2)\n",
    "print(\"Table with best svr models using 10 points\")\n",
    "display(aux2)\n",
    "\n",
    "df_overhad_comb = pd.merge(df_overhad[0],df_overhad[1],on=\"title\")\n",
    "matplotlib_rc_1()\n",
    "df_overhad_comb[[\"title\",\"mpe_y\",\"mpe_x\"]].plot.bar()\n",
    "mpe_mod, mpe_svr= df_overhad_comb[[\"title\",\"mpe_x\",\"mpe_y\"]].mean()\n",
    "plt.plot([-1,13],[mpe_svr,mpe_svr],\"--\")\n",
    "plt.plot([-1,13],[mpe_mod,mpe_mod],\"--\")\n",
    "locs, labels = xticks()\n",
    "ylabel(\"Mean percentage error\")\n",
    "xticks(ticks=locs,labels=df_overhad_comb[\"title\"].values)\n",
    "legend([\"Mean SVR\",\"Mean model\",\"SVR\",\"Model\"])\n",
    "tight_layout()\n",
    "savefig(\"models/mpe_svr_eq.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPE vs Train size (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "min_mpe_svr= df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"svr\")].groupby([\"title\"]).mpe.min().values\n",
    "aux2= df_models[df_models.mpe.isin(min_mpe_svr)].drop_duplicates(\"title\")\n",
    "\n",
    "min_mpe_eq= df_models[(df_models[\"ts\"]==10)&(df_models[\"type\"]==\"equation\")].groupby([\"title\"]).mpe.min().values\n",
    "aux3= df_models[df_models.mpe.isin(min_mpe_eq)].drop_duplicates(\"title\")\n",
    "\n",
    "avgr_eq= []\n",
    "avgr_svr= []\n",
    "avgr_eq_en= []\n",
    "avgr_svr_en= []\n",
    "tss= []\n",
    "\n",
    "for t1,t2 in zip(aux2.name, aux3.name):\n",
    "    matplotlib_rc_2()\n",
    "    aux= df_models[(df_models[\"name\"]==t1)&(df_models[\"type\"]==\"svr\")&(df_models[\"ts\"]>1)]\n",
    "    aux= aux.drop(columns=\"ref\").groupby([\"ts\"]).min().reset_index()\n",
    "    plt.plot(aux[\"ts\"],aux[\"mpe\"],label=\"SVR\")\n",
    "    \n",
    "    avgr_svr.append(aux[\"mpe\"])\n",
    "    avgr_eq_en.append(aux[\"energy\"])\n",
    "    \n",
    "    aux= df_models[(df_models[\"name\"]==t2)&(df_models[\"type\"]==\"equation\")&(df_models[\"ts\"]>1)]\n",
    "    aux= aux.drop(columns=\"ref\").groupby([\"ts\"]).min().reset_index()\n",
    "    plt.plot(aux[\"ts\"],aux[\"mpe\"],label=\"Model\")\n",
    "    \n",
    "    avgr_eq.append(aux[\"mpe\"])\n",
    "    avgr_svr_en.append(aux[\"energy\"])\n",
    "    tss= aux[\"ts\"]\n",
    "    \n",
    "    ylabel(\"Mean percetage error\")\n",
    "    xlabel(\"Number of samples\")\n",
    "    legend()\n",
    "    \n",
    "    #title(get_title(app_name))\n",
    "    tight_layout()\n",
    "    savefig(f\"models/overhead/{t1}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(avgr_svr,axis=0), np.mean(avgr_eq,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_rc_2()\n",
    "plt.plot(tss,np.mean(avgr_svr,axis=0),label=\"Mean error SVR\")\n",
    "plt.plot(tss,np.mean(avgr_eq,axis=0),label=\"Mean error model\")\n",
    "ylabel(\"Mean percetage error\")\n",
    "xlabel(\"Number of samples\")\n",
    "legend()\n",
    "tight_layout()\n",
    "savefig(\"models/overhead/overall_mpe_10pts.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy vs Train size (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_rc_2()\n",
    "plt.plot(tss,np.mean(avgr_eq_en,axis=0),label=\"Mean energy model and SVR\")\n",
    "ylabel(\"Energy (KJ)\")\n",
    "xlabel(\"Number of samples\")\n",
    "legend()\n",
    "tight_layout()\n",
    "savefig(f\"models/overhead/overall_energy_10pts.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error vs train size using the apps with lowest mpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "min_mpe = df_models.groupby([\"title\",\"type\"]).mpe.min().values\n",
    "\n",
    "split_type= \"halton\"\n",
    "for app_name in df_models[df_models.mpe.isin(min_mpe)].name.unique():\n",
    "    print(app_name)\n",
    "    df1= df_models[df_models[\"name\"]==app_name].drop(columns=\"ref\").groupby([\"ts\",\"type\"]).min().reset_index()\n",
    "    df1= df1[df1[\"ts\"]>1]\n",
    "\n",
    "    matplotlib_rc_2()\n",
    "    fig, ax = subplots()\n",
    "    \n",
    "    x1= df1[ (df1[\"type\"]==\"equation\") ][[\"ts\",\"mpe\"]]\n",
    "    x2= df1[ (df1[\"type\"]==\"svr\") ][[\"ts\",\"mpe\"]]\n",
    "    \n",
    "    plot(x1[\"ts\"],x1[\"mpe\"],label=\"Equation\",c=\"b\")\n",
    "    plot(x2[\"ts\"],x2[\"mpe\"],label=\"SVR\",c=\"r\")\n",
    "\n",
    "\n",
    "    ylabel(\"Mean percetage error\")\n",
    "    xlabel(\"Number of sampels\")\n",
    "    legend()\n",
    "    \n",
    "    #title(get_title(app_name))\n",
    "    tight_layout()\n",
    "    savefig(f\"models/overhead/lowest_mpe/{app_name}_{split_type}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy vs train size using the apps with lowest mpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "split_type= \"random\"\n",
    "for app_name in df_models[\"name\"].unique():\n",
    "    print(app_name)\n",
    "    df1= df_models[df_models[\"name\"]==app_name].groupby([\"ts\",\"type\",\"split_type\"]).mpe.min()\n",
    "    df1= df_models[df_models[\"mpe\"].isin(df1.values)].drop_duplicates()\n",
    "    df1= df1[df1[\"ts\"]>1]\n",
    "    \n",
    "    matplotlib_rc_2()\n",
    "    fig, ax = subplots()\n",
    "\n",
    "    x1= df1[ (df1[\"type\"]==\"equation\")&(df1[\"split_type\"]==split_type) ][[\"ts\",\"energy\"]].sort_values(\"ts\")\n",
    "    x2= df1[ (df1[\"type\"]==\"svr\")&(df1[\"split_type\"]==split_type) ][[\"ts\",\"energy\"]].sort_values(\"ts\")\n",
    "\n",
    "    plot(x1[\"ts\"],x1[\"energy\"],label=\"Equation\",c=\"b\")\n",
    "    plot(x2[\"ts\"],x2[\"energy\"],label=\"Svr\",c=\"r\")\n",
    "\n",
    "\n",
    "    ylabel(\"Energy (KJ)\")\n",
    "    xlabel(\"Train size\")\n",
    "    legend()\n",
    "    \n",
    "    title(get_title(app_name))\n",
    "    tight_layout()\n",
    "    #savefig(f\"figures/overhead/svr_vs_eq/{app_name}_{split_type}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "min_mpe = df_models.groupby([\"title\",\"type\"]).mpe.min().values\n",
    "dfz = df_models[df_models[\"name\"].isin(df_models[df_models.mpe.isin(min_mpe)].name.unique())]\n",
    "\n",
    "matplotlib_rc_2()\n",
    "x1 = dfz[ (dfz[\"ts\"]>1)&(dfz[\"type\"]==\"equation\") ].groupby(\"ts\").mpe.mean().plot(label=\"Equation\")\n",
    "xlabel(\"Train size\")\n",
    "ylabel(\"Mean percentage error\")\n",
    "x1 = dfz[ (dfz[\"ts\"]>1)&(dfz[\"type\"]==\"svr\") ].groupby(\"ts\").mpe.mean().plot(label=\"SVR\")\n",
    "xlabel(\"Train size\")\n",
    "ylabel(\"Mean percentage error\")\n",
    "legend()\n",
    "savefig(\"models/overall_lowest_mpe.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = create_df_models()\n",
    "\n",
    "matplotlib_rc_2()\n",
    "x1 = df_models[df_models[\"ts\"]>1].groupby(\"ts\").energy.mean().plot()\n",
    "xlabel(\"Train size\")\n",
    "ylabel(\"Energy (KJ)\")\n",
    "tight_layout()\n",
    "savefig(\"models/overhead/overall_energy_lowest_mpe.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ondemand comparision\n",
    "\n",
    "- Energy saving\n",
    "- Time penality\n",
    "- Table with mean runs to compensante energy spent on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ond = pd.read_csv(\"databases/dvfs_ond_cmp.csv\",index_col=0)\n",
    "ond = ond.loc[set(ond.index)-set([0,4,7,13,14])]\n",
    "display(ond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ond = pd.read_csv(\"databases/dvfs_ond_cmp.csv\",index_col=0)\n",
    "ond = ond.loc[set(ond.index)-set([0,4,7,13,14])]\n",
    "\n",
    "matplotlib_rc_1()\n",
    "ond.sav_max.plot.bar()\n",
    "med= ond.sav_max.mean()\n",
    "print(med)\n",
    "plot([-1,13],[med,med],\"--\",c=\"r\")\n",
    "locs, labels = xticks()\n",
    "ylabel(\"Relative saving\", )\n",
    "yticks()\n",
    "xticks(ticks=locs,labels=ond[\"app\"].values)\n",
    "legend([\"Mean saving\"])\n",
    "tight_layout()\n",
    "savefig(\"models/dvfs_cmp_max.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ond = pd.read_csv(\"databases/dvfs_ond_cmp.csv\",index_col=0)\n",
    "ond = ond.loc[set(ond.index)-set([0,4,7,13,14])]\n",
    "\n",
    "matplotlib_rc_1()\n",
    "ond.sav_mean.plot.bar()\n",
    "med= ond.sav_mean.mean()\n",
    "print(med)\n",
    "plot([-1,13],[med,med],\"--\",c=\"r\")\n",
    "locs, labels = xticks()\n",
    "ylabel(\"Relative saving\")\n",
    "yticks()\n",
    "xticks(ticks=locs,labels=ond[\"app\"].values)\n",
    "legend([\"Mean saving\"])\n",
    "tight_layout()\n",
    "savefig(\"models/dvfs_cmp_mean.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ond = pd.read_csv(\"databases/dvfs_ond_cmp.csv\",index_col=0)\n",
    "ond = ond.loc[set(ond.index)-set([0,4,7,13,14])]\n",
    "\n",
    "matplotlib_rc_1()\n",
    "ond.sav_32.plot.bar()\n",
    "med= ond.sav_32.mean()\n",
    "print(med)\n",
    "plot([-1,13],[med,med],\"--\",c=\"r\")\n",
    "locs, labels = xticks()\n",
    "ylabel(\"Relative saving\")\n",
    "yticks()\n",
    "xticks(ticks=locs,labels=ond[\"app\"].values)\n",
    "legend([\"Mean saving\"])\n",
    "tight_layout()\n",
    "savefig(\"models/dvfs_cmp_32.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "lcolors = prop_cycle.by_key()['color']\n",
    "\n",
    "class MyScalarFormatter(ScalarFormatter):\n",
    "    def __call__(self, x, pos=None):\n",
    "        if len(self.locs) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            xp = (x - self.offset) / (10. ** self.orderOfMagnitude)\n",
    "            if abs(xp) < 1e-8:\n",
    "                xp = 0\n",
    "            if self._useLocale:\n",
    "                s = locale.format_string(self.format, (xp,), grouping=True)\n",
    "            else:\n",
    "                s = self.format % xp\n",
    "            return self.fix_minus(s)\n",
    "\n",
    "locale._override_localeconv = {'thousands_sep': ',', 'grouping': [3,0]}\n",
    "sf = MyScalarFormatter(useLocale=True)\n",
    "\n",
    "xs = [1,0.29,0.97,198,0.8]\n",
    "\n",
    "time_= lambda f,p: xs[0]*( xs[4]/p-xs[4]+1 )/f\n",
    "power_= lambda f,p: (xs[1]*f**3+xs[2]*f)*p+xs[3]\n",
    "energy_= lambda f,p: time_(f,p)*power_(f,p)\n",
    "\n",
    "\n",
    "def pareto_frontier_selection(costs, return_mask= True):  # <- Fastest for many points\n",
    "    #pprint('pareto_frontier_selection')\n",
    "\n",
    "    is_efficient = np.arange(costs.shape[0])#Return evenly spaced values within a given interval.\n",
    "    n_points = costs.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "\n",
    "    while next_point_index<len(costs):#it stops when the number of pareto points found is lower the number of remaining costs\n",
    "        nondominated_point_mask = np.any(costs<=costs[next_point_index], axis=1)#Checking whether any [time,energy] costs<=costs[next_point_index] in each row                  \n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points            \n",
    "        costs = costs[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1#count how many points are non dominated. It advance for the next point that is lower in any column than the current\n",
    "\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [100,0.29,0.97,198,0.8]\n",
    "xx = []\n",
    "yy = []\n",
    "y_desc = 0\n",
    "matplotlib_rc_2()\n",
    "i = 0\n",
    "for w in [0.1,0.3,0.6,0.8,0.9]:\n",
    "    xs[4] = w\n",
    "    f= np.arange(1.2,2.3,0.1)\n",
    "    p= np.arange(1,32,1)\n",
    "    Y, X= np.meshgrid(p,f)\n",
    "    x, y = time_(X,Y), energy_(X,Y)\n",
    "    for v,z in zip(x,y):\n",
    "        scatter(v,z,c=lcolors[i%len(lcolors)],s=1, marker=\".\")\n",
    "    xx = list(x.reshape(-1))\n",
    "    yy = list(y.reshape(-1))\n",
    "    costs = np.stack([xx,yy]).T\n",
    "    mask = pareto_frontier_selection(costs)\n",
    "    scatter(costs[mask][:, 0], costs[mask][:, 1], s=40, label=f\"{w}\", c=lcolors[i%len(lcolors)], marker=\"x\")\n",
    "    \n",
    "    min_idx = np.argmin(costs[:, 1])\n",
    "    fmin, pmin = X.reshape(-1)[min_idx], Y.reshape(-1)[min_idx]\n",
    "    #print(energy_(fmin,pmin) == yy[min_idx])\n",
    "    y_desc = 25000+3000*i\n",
    "    plt.annotate(f\"({fmin:.2f} (GHz), #{pmin:.0f})\", xy=(costs[min_idx, 0],costs[min_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[min_idx, 0]+30,costs[min_idx, 1]), textcoords='data',\n",
    "                arrowprops=dict(facecolor='red', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    \n",
    "    max_idx = np.argmax(costs[:, 1])\n",
    "    fmax, pmax = X.reshape(-1)[max_idx], Y.reshape(-1)[max_idx]\n",
    "    #print(energy_(fmax,pmax), yy[max_idx])\n",
    "    plt.annotate(f\"({fmax:.2f} (GHz), #{pmax:.0f})\", xy=(costs[max_idx, 0],costs[max_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[max_idx, 0]-20,costs[max_idx, 1]), textcoords='data',\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    i += 1\n",
    "\n",
    "ylabel(\"Energy (J)\")\n",
    "xlabel(\"Time (s)\")\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(sf)\n",
    "#ax.set_yscale('log')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "legend(by_label.values(), by_label.keys())\n",
    "ylim(0,50000)\n",
    "tight_layout()\n",
    "savefig(\"models/analisys/pareto_w_low.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [100,0.29,0.97,198,0.8]\n",
    "xx = []\n",
    "yy = []\n",
    "i = 0\n",
    "matplotlib_rc_2()\n",
    "for w in [0.1,0.3,0.6,0.8,0.9]:\n",
    "    xs[4] = w\n",
    "    f= np.arange(1.2,5.0,0.1)\n",
    "    p= np.arange(1,64,1)\n",
    "    Y, X= np.meshgrid(p,f)\n",
    "    x, y = time_(X,Y), energy_(X,Y)\n",
    "    for v,z in zip(x,y):\n",
    "        scatter(v,z,c=lcolors[i%len(lcolors)],s=1, marker=\".\")\n",
    "    xx = list(x.reshape(-1))\n",
    "    yy = list(y.reshape(-1))\n",
    "    costs = np.stack([xx,yy]).T\n",
    "    mask = pareto_frontier_selection(costs)\n",
    "    scatter(costs[mask][:, 0], costs[mask][:, 1], s=40, label=f\"{w}\", c=lcolors[i%len(lcolors)], marker=\"x\")\n",
    "    \n",
    "    min_idx = np.argmin(costs[:, 1])\n",
    "    fmin, pmin = X.reshape(-1)[min_idx], Y.reshape(-1)[min_idx]\n",
    "    #print(energy_(fmin,pmin) == yy[min_idx])\n",
    "    y_desc = 3000*i\n",
    "#     if i == 0:\n",
    "#         y_desc = 3000\n",
    "    plt.annotate(f\"({fmin:.2f} (GHz), #{pmin:.0f})\", xy=(costs[min_idx, 0],costs[min_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[min_idx, 0]+65,costs[min_idx, 1]+y_desc), textcoords='data',\n",
    "                arrowprops=dict(facecolor='red', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    \n",
    "    max_idx = np.argmax(costs[:, 1])\n",
    "    fmax, pmax = X.reshape(-1)[max_idx], Y.reshape(-1)[max_idx]\n",
    "    #print(energy_(fmax,pmax), yy[max_idx])\n",
    "    y_desc = -20 if i > 2 else 40\n",
    "    plt.annotate(f\"({fmax:.2f} (GHz), #{pmax:.0f})\", xy=(costs[max_idx, 0],costs[max_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[max_idx, 0]+y_desc,costs[max_idx, 1]), textcoords='data',\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    i += 1\n",
    "    \n",
    "ylabel(\"Energy (J)\")\n",
    "xlabel(\"Time (s)\")\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(sf)\n",
    "#ax.set_yscale('log')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "legend(by_label.values(), by_label.keys())\n",
    "ylim(0,50000)\n",
    "tight_layout()\n",
    "savefig(\"models/analisys/pareto_w_high.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "i = 0\n",
    "xs = [100,0.29,0.97,198,0.86]\n",
    "y_desc = 0\n",
    "matplotlib_rc_2()\n",
    "for w in [50,100,200,300]:\n",
    "    xs[3] = w\n",
    "    f= np.arange(1.2,2.3,0.1)\n",
    "    p= np.arange(1,32,1)\n",
    "    Y, X= np.meshgrid(p,f)\n",
    "    x, y = time_(X,Y), energy_(X,Y)\n",
    "    \n",
    "    for v,z in zip(x,y):\n",
    "        scatter(v,z,c=lcolors[i%len(lcolors)],s=0.05)\n",
    "    \n",
    "    xx = list(x.reshape(-1))\n",
    "    yy = list(y.reshape(-1))\n",
    "    costs = np.stack([xx,yy]).T\n",
    "    mask = pareto_frontier_selection(costs)\n",
    "    scatter(costs[mask][:, 0], costs[mask][:, 1], s=20, label=f\"{w}\", c=lcolors[i%len(lcolors)])\n",
    "    \n",
    "    min_idx = np.argmin(costs[:, 1])\n",
    "    fmin, pmin = X.reshape(-1)[min_idx], Y.reshape(-1)[min_idx]\n",
    "    #print(energy_(fmin,pmin) == yy[min_idx])\n",
    "    y_desc = 3000*i\n",
    "    plt.annotate(f\"({fmin:.2f} (GHz), #{pmin:.0f})\", xy=(costs[min_idx, 0],costs[min_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[min_idx, 0]+30,costs[min_idx, 1]+y_desc), textcoords='data',\n",
    "                arrowprops=dict(facecolor='red', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    \n",
    "    max_idx = np.argmax(costs[:, 1])\n",
    "    fmax, pmax = X.reshape(-1)[max_idx], Y.reshape(-1)[max_idx]\n",
    "    #print(energy_(fmax,pmax), yy[max_idx])\n",
    "    plt.annotate(f\"({fmax:.2f} (GHz), #{pmax:.0f})\", xy=(costs[max_idx, 0],costs[max_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[max_idx, 0]-10,costs[max_idx, 1]), textcoords='data',\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "ylabel(\"Energy (J)\")\n",
    "xlabel(\"Time (s)\")\n",
    "ax = plt.gca()\n",
    "#ax.set_yscale('log')\n",
    "ax.yaxis.set_major_formatter(sf)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "legend(by_label.values(), by_label.keys())\n",
    "ylim(0,50000)\n",
    "tight_layout()\n",
    "savefig(\"models/analisys/pareto_static_low.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "i = 0\n",
    "xs = [100,0.29,0.97,198,0.86]\n",
    "y_desc = 0\n",
    "matplotlib_rc_2()\n",
    "for w in [50,100,200,300]:\n",
    "    xs[3] = w\n",
    "    f= np.arange(1.2,5.0,0.1)\n",
    "    p= np.arange(1,64,1)\n",
    "    Y, X= np.meshgrid(p,f)\n",
    "    x, y = time_(X,Y), energy_(X,Y)\n",
    "    \n",
    "    for v,z in zip(x,y):\n",
    "        scatter(v,z,c=lcolors[i%len(lcolors)],s=0.05)\n",
    "    \n",
    "    xx = list(x.reshape(-1))\n",
    "    yy = list(y.reshape(-1))\n",
    "    costs = np.stack([xx,yy]).T\n",
    "    mask = pareto_frontier_selection(costs)\n",
    "    scatter(costs[mask][:, 0], costs[mask][:, 1], s=20, label=f\"{w}\", c=lcolors[i%len(lcolors)])\n",
    "    \n",
    "    min_idx = np.argmin(costs[:, 1])\n",
    "    fmin, pmin = X.reshape(-1)[min_idx], Y.reshape(-1)[min_idx]\n",
    "    #print(energy_(fmin,pmin) == yy[min_idx])\n",
    "    y_desc = 1000*i\n",
    "    plt.annotate(f\"({fmin:.2f} (GHz), #{pmin:.0f})\", xy=(costs[min_idx, 0],costs[min_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[min_idx, 0]+40,costs[min_idx, 1]+y_desc), textcoords='data',\n",
    "                arrowprops=dict(facecolor='red', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    \n",
    "    max_idx = np.argmax(costs[:, 1])\n",
    "    fmax, pmax = X.reshape(-1)[max_idx], Y.reshape(-1)[max_idx]\n",
    "    #print(energy_(fmax,pmax), yy[max_idx])\n",
    "    y_desc = 25 if i == 0 else -20\n",
    "    plt.annotate(f\"({fmax:.2f} (GHz), #{pmax:.0f})\", xy=(costs[max_idx, 0],costs[max_idx, 1]),\n",
    "                xycoords='data', xytext=(costs[max_idx, 0]+y_desc,costs[max_idx, 1]), textcoords='data',\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.1, width=0.1, headwidth=5),\n",
    "                horizontalalignment='right', verticalalignment='top', color=lcolors[i%len(lcolors)], fontsize=14)\n",
    "    i += 1\n",
    "\n",
    "ylabel(\"Energy (J)\")\n",
    "xlabel(\"Time (s)\")\n",
    "ax = plt.gca()\n",
    "#ax.set_yscale('log')\n",
    "ax.yaxis.set_major_formatter(sf)\n",
    "\n",
    "matplotlib.ticker.FuncFormatter(lambda x, p: format(x, ',.6g').replace(',', ' '))\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "legend(by_label.values(), by_label.keys())\n",
    "ylim(0,50000)\n",
    "tight_layout()\n",
    "savefig(\"models/analisys/pareto_static_high.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying the input size parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of instructions is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        if len(df[\"frequency\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "            df = df[df[\"sensors\"] == \"fingerprint_sample\"]\n",
    "            df[\"info\"] = df[\"info\"].apply(flat)\n",
    "            df = df[df[\"repetitions\"] == \"2\"]\n",
    "            matplotlib_rc_2()\n",
    "            figure()\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            print(fname)\n",
    "            for freq in df[\"frequency\"].values:\n",
    "                y = df[df[\"frequency\"]==freq][\"info\"].values\n",
    "                y = list(y[0])\n",
    "                y = np.diff(y)\n",
    "\n",
    "                if len(y) > 3:\n",
    "                    sz = 1\n",
    "                    if True:\n",
    "                        sz = len(y)\n",
    "                    npoints = 100\n",
    "                    x0, y0= np.linspace(0,sz,len(y)), y\n",
    "                    tck = interpolate.splrep(x0, y0, s=0)\n",
    "                    x1 = np.linspace(0,sz,npoints)\n",
    "                    y1 = interpolate.splev(x1, tck, der=0)\n",
    "                    y = savgol_filter(y1,11,3)\n",
    "                    xlabel(\"Time\")\n",
    "                    ylabel(\"Instructions/seconds\")\n",
    "                    #y1 = y1*2.2/freq\n",
    "                    plot(x1, y1, label=freq)\n",
    "\n",
    "            legend()\n",
    "            tight_layout()\n",
    "            savefig(f\"models/hypothesis/const_intructions/freq/{fname}.pdf\")\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        if len(df[\"cores\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            df = data.dataframe_group(\"sensors\")\n",
    "            df = df[df[\"sensors\"] == \"fingerprint_sample\"]\n",
    "            df[\"info\"] = df[\"info\"].apply(flat)\n",
    "\n",
    "            df = df[df[\"repetitions\"] == \"1\"]\n",
    "            matplotlib_rc_2()\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            print(fname)\n",
    "            for freq in df[\"cores\"].values:\n",
    "                y = df[df[\"cores\"]==freq][\"info\"].values\n",
    "                y = list(y[0])\n",
    "                y = np.diff(y)\n",
    "\n",
    "                if len(y) > 3:\n",
    "                    sz = 1\n",
    "                    if False:\n",
    "                        sz = len(y)\n",
    "                    npoints = 100\n",
    "                    x0, y0= np.linspace(0,sz,len(y)), y\n",
    "                    tck = interpolate.splrep(x0, y0, s=0)\n",
    "                    x1 = np.linspace(0,sz,npoints)\n",
    "                    y1 = interpolate.splev(x1, tck, der=0)\n",
    "                    y = savgol_filter(y1,5,3)\n",
    "                    xlabel(\"Time\")\n",
    "                    ylabel(\"Instructions/seconds\")\n",
    "                    plot(x0, y0, label=freq)\n",
    "                #plot(y)\n",
    "\n",
    "            legend()\n",
    "            tight_layout()\n",
    "            savefig(f\"models/hypothesis/const_intructions/cores/{fname}.pdf\")\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                #\"pkgs\": {\"$addToSet\": \"$$ROOT\" },\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        if len(df[\"cores\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            \n",
    "            df = data.dataframe_generic()\n",
    "            df[\"time\"] = df[\"stop_time\"]-df[\"start_time\"]\n",
    "            df = df.drop(columns=[\"start_time\", \"stop_time\"])\n",
    "            df[\"instructions\"] = df[\"fingerprint_sample\"].apply(lambda x : x[-1])\n",
    "            dfx = df.groupby([\"cores\"]).mean()\n",
    "\n",
    "            if \"fluid\" in fname:\n",
    "                # fluid can only run with power of 2\n",
    "                dfx = dfx[dfx[\"time\"] > 1]\n",
    "\n",
    "            if \"x264\" in fname:\n",
    "                # x264 with one core ?\n",
    "                dfx = dfx.loc[dfx.index != \"1\"]\n",
    "\n",
    "            dfx.index = dfx.index.astype(int)\n",
    "            dfx = dfx.sort_index()\n",
    "            res = dfx[\"instructions\"].std()/dfx[\"instructions\"].mean()*100\n",
    "            res_m = dfx[\"instructions\"].mean()\n",
    "            res_s = dfx[\"instructions\"].std()\n",
    "            print(fname.capitalize(), f\"{res_m:.2e} {res_s:.2e} {res:.2f}%\")\n",
    "        #     if res > 4:\n",
    "        #         dfx.plot(y=\"instructions\")\n",
    "        #         show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_generic()\n",
    "        if len(df[\"frequency\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            \n",
    "            df[\"time\"] = df[\"stop_time\"]-df[\"start_time\"]\n",
    "            df = df.drop(columns=[\"start_time\", \"stop_time\"])\n",
    "            df[\"instructions\"] = df[\"fingerprint_sample\"].apply(lambda x : x[-1])\n",
    "            dfx = df.groupby([\"frequency\"]).mean()\n",
    "\n",
    "            if \"fluid\" in fname:\n",
    "                # fluid can only run with power of 2\n",
    "                dfx = dfx[dfx[\"time\"] > 1]\n",
    "\n",
    "            if \"x264\" in fname:\n",
    "                # x264 with one core ?\n",
    "                dfx = dfx.loc[dfx.index != \"1\"]\n",
    "\n",
    "\n",
    "            res = dfx[\"instructions\"].std()/dfx[\"instructions\"].mean()*100\n",
    "            res_m = dfx[\"instructions\"].mean()\n",
    "            res_s = dfx[\"instructions\"].std()\n",
    "            print(fname.capitalize(), f\"{res_m:.2e} {res_s:.2e} {res:.2f}%\")\n",
    "        #     if res > 1:\n",
    "        #         dfx.plot(y=\"instructions\")\n",
    "        #         show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input size and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "covs = []\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "#         {\n",
    "#             \"$match\":{\n",
    "#                 \"config.pkg\": \"./x264\"\n",
    "#             }\n",
    "#         },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_generic()\n",
    "        if len(df[\"input\"].unique()) > 1:# and \"1\" in data.config[\"command\"]:\n",
    "            df[\"time\"] = df[\"stop_time\"]-df[\"start_time\"]\n",
    "            df[\"instructions\"] = df[\"fingerprint_sample\"].apply(lambda x : x[-1])\n",
    "            df = df.drop(columns=[\"start_time\", \"stop_time\", \"fingerprint_sample\"])\n",
    "            df = df.groupby([\"input\"]).mean().reset_index()\n",
    "            df = df.sort_values(\"input\").astype(float)\n",
    "\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            matplotlib_rc_2()\n",
    "            plot(df[\"time\"], df[\"instructions\"])\n",
    "            print(np.corrcoef(df[\"time\"], df[\"instructions\"]))\n",
    "            covs.append([fname.capitalize(),np.corrcoef(df[\"time\"], df[\"instructions\"])[0][1]])\n",
    "            scatter(df[\"time\"], df[\"instructions\"])\n",
    "            xlabel(\"Time (s)\")\n",
    "            ylabel(\"Instructions\")\n",
    "            tight_layout()\n",
    "            savefig(f\"models/hypothesis/input_instructions/input_time/{fname}.pdf\")\n",
    "            show()\n",
    "covs = pd.DataFrame(covs, columns=[\"name\",\"cov\"])\n",
    "covs.groupby(\"name\").max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_generic()\n",
    "        if len(df[\"input\"].unique()) > 1 and len(df[\"frequency\"].unique()) > 1:\n",
    "            df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "            df[\"time\"] = df[\"stop_time\"]-df[\"start_time\"]\n",
    "            df[\"instructions\"] = df[\"fingerprint_sample\"].apply(lambda x : x[-1])\n",
    "            df = df.drop(columns=[\"start_time\", \"stop_time\", \"fingerprint_sample\"])\n",
    "            df = df.groupby([\"input\", \"frequency\"]).mean().reset_index()\n",
    "            df = df.sort_values(\"input\").astype(float)\n",
    "\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            matplotlib_rc_2()\n",
    "            for fx in sorted(df[\"frequency\"].unique()):\n",
    "                dfx = df[df[\"frequency\"]==fx]\n",
    "                plot(dfx[\"time\"], dfx[\"instructions\"],label=fx)\n",
    "            xlabel(\"Time (s)\")\n",
    "            ylabel(\"Instructions\")\n",
    "            legend()\n",
    "            tight_layout()\n",
    "            savefig(f\"models/hypothesis/input_instructions/input_time/{fname}.pdf\")\n",
    "            show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprint parsec applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True\n",
    "    ):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        if len(df[\"input\"].unique()) > 1 and \"1\" in data.config[\"command\"]:\n",
    "            df = df[df[\"sensors\"]==\"fingerprint_sample\"]\n",
    "            df[\"info\"] = df[\"info\"].apply(flat)\n",
    "            df[\"time\"] = df[\"time\"].apply(lambda x: x[-1]-x[0])\n",
    "            df = df[df[\"repetitions\"]==\"2\"]\n",
    "\n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            matplotlib_rc_2()\n",
    "            print(fname)\n",
    "            for i, v in df.iterrows():\n",
    "                y = np.diff(v[\"info\"])\n",
    "                sz = 1\n",
    "                if True:\n",
    "                    sz = len(y)\n",
    "                npoints = 100\n",
    "                if len(y) > 3:\n",
    "                    x0, y0= np.linspace(0,sz,len(y)), y\n",
    "                    tck = interpolate.splrep(x0, y0, s=0)\n",
    "                    x1 = np.linspace(0,sz,npoints)\n",
    "                    y1 = interpolate.splev(x1, tck, der=0)\n",
    "                    y = savgol_filter(y1,5,3)\n",
    "                plot(y, label= \"input \"+v[\"input\"])\n",
    "\n",
    "            xlabel(\"Percentage of execution (%)\")\n",
    "            ylabel(\"Instructions/seconds\")\n",
    "            legend()\n",
    "            tight_layout()\n",
    "            savefig(f\"models/hypothesis/input_instructions/fp/{fname}.pdf\")\n",
    "            show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptype= False\n",
    "show_contour, show_arrow= True, False\n",
    "\n",
    "cmap= cm.hot.reversed()\n",
    "\n",
    "f,p= sympy.symbols(\"f p\")\n",
    "xs= [1,0.29,0.97,198,0.5]\n",
    "\n",
    "show_p= True\n",
    "xs= [1,0.29,0.97,198,0.5]\n",
    "func= xs[0]*(((xs[1]*f**3+xs[2]*f)*p+xs[3])*(xs[4]/p-xs[4]+1))/f\n",
    "fmax= 2.2\n",
    "tmax= 16\n",
    "\n",
    "def draw(name1, name2):\n",
    "    matplotlib_rc_2()\n",
    "    fig = plt.figure()\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    x1= np.linspace(1,fmax,10)\n",
    "    x2= np.linspace(1,tmax,10)\n",
    "    X,Y= np.meshgrid(x1,x2)\n",
    "\n",
    "    func= xs[0]*(((xs[1]*f**3+xs[2]*f)*p+xs[3])*(xs[4]/p-xs[4]+1))/f\n",
    "    grad= [sympy.diff(func, var) for var in (f,p)]\n",
    "    t= sympy.lambdify([f,p],func)\n",
    "\n",
    "    Z= t(X,Y)\n",
    "\n",
    "    if ptype:\n",
    "        U, V= np.meshgrid(x1,x2)\n",
    "        ax.plot_surface(X, Y, Z, antialiased=True, cmap=cmap)\n",
    "        W= Z\n",
    "    else:\n",
    "        U, V, W= np.meshgrid(x1,x2,np.linspace(0,1,5))\n",
    "\n",
    "    g= sympy.lambdify([f,p],grad)\n",
    "    G= g(U,V)\n",
    "    u = -G[0]\n",
    "    v = -G[1]\n",
    "    w = -t(u,v)\n",
    "\n",
    "    ax.quiver(U, V, W, u, v, w, length=0.1, color=\"r\", normalize=True)\n",
    "\n",
    "    c= ax2.pcolormesh(x1, x2, Z, cmap=cmap)\n",
    "    G= g(X,Y)\n",
    "    u = -G[0]/np.hypot(G[0],G[1])\n",
    "    v = -G[1]/np.hypot(G[0],G[1])\n",
    "\n",
    "    if show_arrow:\n",
    "        ax2.quiver(X, Y, u, v, color=\"b\")\n",
    "    if show_contour:\n",
    "        ax2.contour(X,Y,Z,50)\n",
    "    \n",
    "    \n",
    "    ax.set_ylabel(\"Cores\")\n",
    "    ax.set_xlabel(\"Frequency (GHz)\")\n",
    "    ax.set_zlabel(\"Energy (KJ)\")\n",
    "    \n",
    "    ax2.set_ylabel(\"Cores\")\n",
    "    ax2.set_xlabel(\"Frequency (GHz)\")\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "    fig2.canvas.draw_idle()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(name1)\n",
    "    fig2.tight_layout()\n",
    "    fig2.savefig(name2)\n",
    "    show()\n",
    "\n",
    "xs[4] = 1\n",
    "draw(\"models/analisys/w1_3d.pdf\", \"models/analisys/w1.pdf\")\n",
    "xs[4] = 0\n",
    "draw(\"models/analisys/w0_3d.pdf\", \"models/analisys/w0.pdf\")\n",
    "xs[4] = 0.5\n",
    "\n",
    "xs[3] = 0\n",
    "draw(\"models/analisys/pstatic0_3d.pdf\", \"models/analisys/pstatic0.pdf\")\n",
    "xs[3] = 3000\n",
    "draw(\"models/analisys/pstatic3000_3d.pdf\", \"models/analisys/pstatic3000.pdf\")\n",
    "xs[3] = 198\n",
    "\n",
    "xs[2] = 0\n",
    "draw(\"models/analisys/pleak0_3d.pdf\", \"models/analisys/pleak0.pdf\")\n",
    "xs[2] = 10\n",
    "draw(\"models/analisys/pleak10_3d.pdf\", \"models/analisys/pleak10.pdf\")\n",
    "xs[2] = 0.97\n",
    "\n",
    "xs[1] = 0\n",
    "draw(\"models/analisys/pdyn0_3d.pdf\", \"models/analisys/pdyn0.pdf\")\n",
    "xs[1] = 3\n",
    "draw(\"models/analisys/pdyn3_3d.pdf\", \"models/analisys/pdyn3.pdf\")\n",
    "xs[1] = 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(pascalmodel):\n",
    "    df = pascalmodel.data\n",
    "    df[pascalmodel.inputs] = df[pascalmodel.inputs].astype(float)\n",
    "    df_train= df.loc[pascalmodel.train_idx]\n",
    "    # print(pascalmodel.config)\n",
    "\n",
    "    freqs = sort(df_train.frequency.unique().astype(float))\n",
    "    #freqs = np.arange(1.1, 2.3, 0.1)\n",
    "    cores = sort(df_train.cores.unique().astype(int))\n",
    "    #cores = np.arange(1, 33, 1)\n",
    "    matplotlib_rc_2()\n",
    "    plotdata.new_figure()\n",
    "    \n",
    "    best= []\n",
    "    X = np.array(np.meshgrid(cores, freqs)).T.reshape(-1, 2)\n",
    "    Y = pascalmodel.predict(X)\n",
    "    idx = np.argmin(Y)\n",
    "    best= list(X[idx])+[Y[idx]]\n",
    "    renergy= df[(df[\"cores\"]==X[idx][0])&\n",
    "                     (df[\"frequency\"]==X[idx][1])].energy.iloc[0]\n",
    "    print(Y[idx], X[idx])\n",
    "    print(\"Real energy\", renergy)\n",
    "\n",
    "\n",
    "    plotdata.setProps(ylabel='Frequencies (GHz)',\n",
    "                      xlabel='Active threads',\n",
    "                      zlabel='Energy (KJ)')\n",
    "\n",
    "    dfaux = df_train\n",
    "    dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "    plotdata.ax.scatter(dfaux[\"cores\"],\n",
    "                    dfaux[\"frequency\"],\n",
    "                    dfaux[\"energy\"],\n",
    "                    antialiased=True, color=\"red\",s=100)\n",
    "\n",
    "    dfaux = df.loc[set(df.index)-set(pascalmodel.train_idx)]\n",
    "    dfaux = dfaux.sort_values([\"frequency\", \"cores\"])\n",
    "    plotdata.ax.scatter(dfaux[\"cores\"],\n",
    "                    dfaux[\"frequency\"],\n",
    "                    dfaux[\"energy\"],\n",
    "                    antialiased=True, color=\"black\",s=100)\n",
    "\n",
    "    if len(best) > 2:\n",
    "        plotdata.ax.scatter([best[0]],\n",
    "                        [best[1]],\n",
    "                        [best[2]],\n",
    "                        antialiased=True, color=\"y\",s=100)\n",
    "\n",
    "    plotdata.plot3D(cores, freqs, Y, points=False, color_=\"b\")\n",
    "    plotdata.ax.legend([\"train values\",\"measured values\",\"min energy\",\"model\"])\n",
    "\n",
    "    plotdata.ax.view_init(30, 60)\n",
    "    plt.tight_layout()\n",
    "    return int(best[1]*1e6), int(best[0]), renergy, Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"databases/black_pascal_rapl_10M.csv\",index_col=0)\n",
    "\n",
    "opt = LeastSquaresOptmizer(\n",
    "\"\"\"\n",
    "pw_eq= lambda x,f,p: (x[0]*f**3+x[1]*f)*p+x[2]\n",
    "perf_eq= lambda x,f,p: x[1]*(p-x[0]*p+x[0])/(f*p)\n",
    "model= lambda x,p,f: pw_eq(x,f,p)*perf_eq(x[3:],f,p)\n",
    "\"\"\", 5)\n",
    "\n",
    "\n",
    "df1= df[df[\"regions\"]==0]\n",
    "pascalmodel_r1 = create_model(df1,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "df2= df[df[\"regions\"]==1]\n",
    "pascalmodel_r2 = create_model(df2,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "df3= df[df[\"regions\"]==2]\n",
    "pascalmodel_r3 = create_model(df3,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "dfa= df.groupby([\"cores\",\"frequency\"]).sum().reset_index()\n",
    "pascalmodel_a= create_model(dfa,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f1,c1,re1,pe1= plot3d(pascalmodel_r1)\n",
    "savefig(\"phases/manual/blackscholes_1.pdf\")\n",
    "show()\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f2,c2,re2,pe2= plot3d(pascalmodel_r2)\n",
    "savefig(\"phases/manual/blackscholes_2.pdf\")\n",
    "show()\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f3,c3,re3,pe3= plot3d(pascalmodel_r3)\n",
    "savefig(\"phases/manual/blackscholes_3.pdf\")\n",
    "show()\n",
    "\n",
    "f,c,re,pe= plot3d(pascalmodel_a)\n",
    "savefig(\"phases/manual/blackscholes.pdf\")\n",
    "show()\n",
    "\n",
    "b= re1+re2+re3\n",
    "a= re\n",
    "figure(figsize=(8,5))\n",
    "bar([1,2],[a/a,b/a])\n",
    "a, b, b/a\n",
    "savefig(\"phases/manual/blackscholes_cmp.pdf\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openmp instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"databases/black_pascal_rapl_10M_inst_new2.csv\",index_col=0)\n",
    "\n",
    "opt = LeastSquaresOptmizer(\n",
    "\"\"\"\n",
    "pw_eq= lambda x,f,p: (x[0]*f**3+x[1]*f)*p+x[2]\n",
    "perf_eq= lambda x,f,p: x[1]*(p-x[0]*p+x[0])/(f*p)\n",
    "model= lambda x,p,f: pw_eq(x,f,p)*perf_eq(x[3:],f,p)\n",
    "\"\"\", 5)\n",
    "\n",
    "df[\"regions\"] = df[\"regions\"].astype(int)\n",
    "\n",
    "\n",
    "df1= df[df[\"regions\"]==1]\n",
    "pascalmodel_r1 = create_model(df1,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "df2= df[df[\"regions\"]==2]\n",
    "pascalmodel_r2 = create_model(df2,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "df3= df[df[\"regions\"]==3]\n",
    "pascalmodel_r3 = create_model(df3,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "dfa= df.groupby([\"cores\",\"frequency\"]).sum().reset_index()\n",
    "pascalmodel_a= create_model(dfa,\n",
    "                           inputs=[\"cores\", \"frequency\"],\n",
    "                           output=[\"energy\"], model=deepcopy(opt),\n",
    "                           config=None, train_sz=0.9,\n",
    "                           split_type=\"random\")\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f1,c1,re1,pe1= plot3d(pascalmodel_r1)\n",
    "savefig(\"phases/openmp/blackscholes_1.pdf\")\n",
    "show()\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f2,c2,re2,pe2= plot3d(pascalmodel_r2)\n",
    "savefig(\"phases/openmp/blackscholes_2.pdf\")\n",
    "show()\n",
    "\n",
    "val= abs(pascalmodel_r1.predict(df1[[\"cores\", \"frequency\"]].astype(float).values)-df1[\"energy\"])/df1[\"energy\"]\n",
    "print(max(val)*100, np.mean(val)*100, np.min(val)*100)\n",
    "f3,c3,re3,pe3= plot3d(pascalmodel_r3)\n",
    "savefig(\"phases/openmp/blackscholes_3.pdf\")\n",
    "show()\n",
    "\n",
    "f,c,re,pe= plot3d(pascalmodel_a)\n",
    "savefig(\"phases/openmp/blackscholes.pdf\")\n",
    "show()\n",
    "\n",
    "b= re1+re2+re3\n",
    "a= re\n",
    "figure(figsize=(8,5))\n",
    "bar([1,2],[a/a,b/a])\n",
    "print(a, b, b/a)\n",
    "savefig(\"phases/openmp/blackscholes_cmp.pdf\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add how the regions are computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pthread instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO make sure this is pthreads\n",
    "run_col = energydb[\"run\"]\n",
    "cursor = run_col.aggregate([\n",
    "    {\n",
    "        \"$match\": { \"config.data_descriptor.extras.regions.values\": {\"$ne\":None}},\n",
    "    },\n",
    "    {\n",
    "        \"$group\":\n",
    "        {\n",
    "            \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "            \"keys\": {\"$push\": \"$_id\" },\n",
    "            \"nitem\": {\"$sum\": 1}\n",
    "        }\n",
    "    }\n",
    "],  allowDiskUse=True)\n",
    "\n",
    "matplotlib_rc_1()\n",
    "for d in cursor:\n",
    "    for k in d[\"keys\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        try:\n",
    "            df = data.regions(regions=True)\n",
    "#             df = data.dataframe_group(\"regions\")\n",
    "            x = df[ (df[\"cores\"]==\"30\")&(df[\"frequency\"]==\"2200000\")&(df[\"input\"]==\"1\")]\n",
    "            p = 0\n",
    "            for i, v in x.iterrows():\n",
    "                w = np.array(v[\"stop_time\"])-np.array(v[\"start_time\"])\n",
    "                p += 1\n",
    "                plt.barh(y=p, width=w, left=np.array(v[\"start_time\"]))\n",
    "            xlabel(\"Time (s)\")\n",
    "            ylabel(\"Region\")\n",
    "            tight_layout()\n",
    "            fname = data.config[\"pkg\"].strip(\"./\")\n",
    "            savefig(f\"phases/regions_{fname}.pdf\")\n",
    "            show()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "max_en = df_phases.groupby(\"app\").energy.max()\n",
    "min_en = df_phases.groupby(\"app\").energy.min()\n",
    "df_min = pd.merge(df_phases, min_en).drop_duplicates(\"app\")\n",
    "df_max = pd.merge(df_phases, max_en).drop_duplicates(\"app\")\n",
    "\n",
    "en_diff = (max_en-min_en)/min_en*100\n",
    "print(\"Min energy number of phases in avgr\", df_min[\"nphases\"].mean())\n",
    "print(\"Max energy number of phases in avgr\", df_max[\"nphases\"].mean())\n",
    "print(\"Energy difference\", en_diff.mean(), \"%\")\n",
    "\n",
    "print(\"Min energy phase\")\n",
    "matplotlib_rc_1()\n",
    "df_min[\"nphases\"].hist()\n",
    "xlabel(\"Number of phases for minimal energy\")\n",
    "ylabel(\"Frequency\")\n",
    "tight_layout()\n",
    "savefig(\"phases/min_phases_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "matplotlib_rc_1()\n",
    "print(\"35 Phase\")\n",
    "dfx = df_phases[df_phases[\"nphases\"]==36].reset_index(drop=True)\n",
    "for i, v in dfx.iterrows():\n",
    "    x = json.loads(v[\"phases\"].replace(\"'\", \"\\\"\"))\n",
    "    x = np.array(x,dtype=float)\n",
    "    y = [0,1]*len(x)\n",
    "    for s in x[::-1]:\n",
    "        barh(i, s, 0.8)\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "x = yticks(dfx.index, labels=dfx[\"app\"])\n",
    "yticks(fontsize=14)\n",
    "xlabel(\"Division\")\n",
    "ylabel(\"App\")\n",
    "tight_layout()\n",
    "savefig(\"phases/phase_division_35.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "matplotlib_rc_1()\n",
    "print(\"3 Phase\")\n",
    "dfx = df_phases[df_phases[\"nphases\"]==6].reset_index(drop=True)\n",
    "for i, v in dfx.iterrows():\n",
    "    x = json.loads(v[\"phases\"].replace(\"'\", \"\\\"\"))\n",
    "    x = np.array(x,dtype=float)\n",
    "    y = [0,1]*len(x)\n",
    "    for s in x[::-1]:\n",
    "        barh(i, s, 0.8)\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "x = yticks(dfx.index, labels=dfx[\"app\"])\n",
    "yticks(fontsize=14)\n",
    "xlabel(\"Division\")\n",
    "ylabel(\"App\")\n",
    "tight_layout()\n",
    "savefig(\"phases/phase_division_3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "ax = None\n",
    "matplotlib_rc_1()\n",
    "for appname in df_phases.app.unique():\n",
    "    if \"freq\" in appname: continue\n",
    "    xx= df_phases[(df_phases[\"app\"]==appname)]\n",
    "    xx[\"energy\"] = xx[\"energy\"]/xx[\"energy\"].max()\n",
    "    ax = xx.plot(x=\"nphases\",y=\"energy\",label=appname, ax = ax)\n",
    "    ylabel(\"Relative energy\")\n",
    "    xlabel(\"Number of phases\")\n",
    "    #ylim(0,1)\n",
    "legend(fontsize=10,loc=\"upper right\")\n",
    "tight_layout()\n",
    "savefig(\"fingerprint/energy_per_phase.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_rc_1()\n",
    "df_real_sim = pd.read_csv(\"databases/phases_sim_real.csv\", index_col=0)\n",
    "df_real_sim[\"real_en\"].hist()\n",
    "xlabel(\"Energy (J)\")\n",
    "ylabel(\"Frequency\")\n",
    "tight_layout()\n",
    "savefig(\"phases/cpm_real_sim.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "\n",
    "def get_timeslice_energy(dfx, beg, end):\n",
    "    start_time = dfx[\"start_time\"]\n",
    "    stop_time = dfx[\"stop_time\"]\n",
    "    x = np.array(dfx[\"info\"])\n",
    "    t = np.array(dfx[\"time\"])\n",
    "    \n",
    "    t1 = start_time+(stop_time-start_time)*beg\n",
    "    t2 = start_time+(stop_time-start_time)*end\n",
    "\n",
    "    v1 = np.argmax(t>=t1)-1\n",
    "    v1 = v1 if v1 >= 0 else 0\n",
    "    v2 = np.argmax(t>=t2)-1\n",
    "    v2 = v2 if v2 >= 0 else 0\n",
    "\n",
    "    if v1 == v2:\n",
    "        return t2-t1, (t2-t1)*(x[v1])\n",
    "\n",
    "    en = (t[v1+1]-t1)*x[v1]+(t2-t[v2])*x[v2]\n",
    "    if v1+1<v2:\n",
    "        tt = t[v1+1:v2+1]\n",
    "        tt = tt[1:]-tt[:-1]\n",
    "        xx = x[v1+1:v2+1][:-1]\n",
    "        en += sum(xx*tt)\n",
    "    return t2-t1, en\n",
    "\n",
    "def plot_best(dfx, bps, app=\"\", cores_freqs= False):\n",
    "    pf = []\n",
    "    pc = []\n",
    "    for i, (a, b) in enumerate(zip(bps[:-1], bps[1:])):\n",
    "        min_en = {}\n",
    "        for i, v in dfx.iterrows():\n",
    "            t, e = get_timeslice_energy(v, a, b)\n",
    "            if e >= 0:\n",
    "                f, p = v[\"frequency\"], v[\"cores\"]\n",
    "                min_en[f\"{f};{p}\"] = e\n",
    "        \n",
    "        conf = min(min_en, key=min_en.get)\n",
    "        pf.append(float(conf.split(\";\")[0]))\n",
    "        pc.append(int(conf.split(\";\")[1]))\n",
    "\n",
    "    nx = np.repeat(bps,2)[1:-1]*100\n",
    "    \n",
    "    if cores_freqs:\n",
    "        ny = np.repeat(pc,2)\n",
    "        plot(nx, ny, alpha=1.0, label=f\"{len(bps)}_freq\")\n",
    "    else:\n",
    "        ny = np.repeat(pc,2)\n",
    "        plot(nx, ny, alpha=1.0, label=f\"{len(bps)}_cores\")\n",
    "\n",
    "run_col = energydb[\"run\"]\n",
    "for appname in df_phases.app.unique():\n",
    "    print(appname[:-5])\n",
    "    for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.pkg\": appname[:-5]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True\n",
    "    ):\n",
    "        for k in d[\"ids\"]:\n",
    "            if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "                data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "                data[\"_id\"]= str(data[\"_id\"])\n",
    "                json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "            data = PascalData(f\"cache/{k}.json\")\n",
    "    dfx = data.dataframe_group(\"sensors\")\n",
    "    dfx[\"start_time\"] = dfx[\"time\"].apply(lambda x: x[0])\n",
    "    dfx[\"stop_time\"] = dfx[\"time\"].apply(lambda x: x[-1])\n",
    "    dfx[\"total_time\"] = dfx[\"stop_time\"]-dfx[\"start_time\"]\n",
    "    #df= df[df[\"frequency\"]!=\"2300000\"]\n",
    "    dfx = dfx[dfx[\"input\"] == dfx[\"input\"].unique()[-1]]\n",
    "    \n",
    "    \n",
    "    aux = df_phases[(df_phases[\"app\"]==appname)&(df_phases[\"nphases\"]==37)][\"phases\"].values[0]\n",
    "    phases_aux = list(map(float,json.loads(aux.replace(\"'\", \"\\\"\"))))\n",
    "    plot_best(dfx, phases_aux, appname, True)\n",
    "    \n",
    "    max_phases = df_phases[df_phases[\"app\"]==appname][\"nphases\"].max()\n",
    "    aux = df_phases[(df_phases[\"app\"]==appname)&(df_phases[\"nphases\"]==max_phases)][\"phases\"].values[0]\n",
    "    phases_aux = list(map(float,json.loads(aux.replace(\"'\", \"\\\"\"))))\n",
    "    plot_best(dfx, phases_aux, appname, True)\n",
    "    \n",
    "    xlabel(\"Percentage of execution\")\n",
    "    ylabel(\"Number of cores\")\n",
    "    legend()\n",
    "    savefig(f\"phases/signals/{appname[:-5]}_cores_signals_cmp.pdf\")\n",
    "    show()\n",
    "    \n",
    "    aux = df_phases[(df_phases[\"app\"]==appname)&(df_phases[\"nphases\"]==37)][\"phases\"].values[0]\n",
    "    phases_aux = list(map(float,json.loads(aux.replace(\"'\", \"\\\"\"))))\n",
    "    plot_best(dfx, phases_aux, appname, False)\n",
    "    \n",
    "    max_phases = df_phases[df_phases[\"app\"]==appname][\"nphases\"].max()\n",
    "    aux = df_phases[(df_phases[\"app\"]==appname)&(df_phases[\"nphases\"]==max_phases)][\"phases\"].values[0]\n",
    "    phases_aux = list(map(float,json.loads(aux.replace(\"'\", \"\\\"\"))))\n",
    "    plot_best(dfx, phases_aux, appname, False)\n",
    "    \n",
    "    xlabel(\"Percentage of execution\")\n",
    "    ylabel(\"Number of cores\")\n",
    "    legend()\n",
    "    savefig(f\"phases/signals/{appname[:-5]}_freq_signals_cmp.pdf\")\n",
    "    show()\n",
    "    \n",
    "#     for i,v in df_phases[df_phases[\"app\"]==appname].iterrows():\n",
    "#         print(v[\"nphases\"])\n",
    "#         phases_aux = list(map(float,json.loads(v[\"phases\"].replace(\"'\", \"\\\"\"))))\n",
    "#         plot_best(dfx, phases_aux, appname, True)\n",
    "#         xlabel(\"Percentage of execution\")\n",
    "#         ylabel(\"Number of cores\")\n",
    "#         legend()\n",
    "#         savefig(f\"phases/{appname[:-5]}_{v['nphases']}_cores_signals.pdf\")\n",
    "#         show()\n",
    "\n",
    "#         plot_best(dfx, phases_aux, appname, False)\n",
    "#         xlabel(\"Percentage of execution\")\n",
    "#         ylabel(\"Frequency (GHz)\")\n",
    "#         legend()\n",
    "#         savefig(f\"phases/{appname[:-5]}_{v['nphases']}_freq_signals.pdf\")\n",
    "#         show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprint Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "\n",
    "nphases = 3\n",
    "lw = 5\n",
    "\n",
    "for f in glob.glob(f\"databases/fingerprints/parsec/*.dat\"):\n",
    "    matplotlib_rc_1()\n",
    "    aname = os.path.basename(f)[:-8]\n",
    "    xx = df_phases[df_phases[\"app\"].str.contains(aname)]\n",
    "    xx = xx[xx[\"nphases\"] == nphases]\n",
    "    print(aname)\n",
    "    xx = xx[\"phases\"].values[-1]\n",
    "    xx = json.loads(xx.replace(\"'\", \"\\\"\"))\n",
    "    x = [float(y) for y in xx]\n",
    "    \n",
    "    fp_data = Analyser(f)\n",
    "    try:\n",
    "        fp_data.df[\"m1\"] = fp_data.df['PERF_COUNT_HW_INSTRUCTIONS']/fp_data.df['MEM_UOPS_RETIRED:ALL_STORES']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m1\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m1\",linewidth=lw)\n",
    "\n",
    "        fp_data.df[\"m2\"] = fp_data.df['PERF_COUNT_HW_INSTRUCTIONS']/fp_data.df['MEM_UOPS_RETIRED:ALL_LOADS']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m2\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m2\",linewidth=lw)\n",
    "\n",
    "        fp_data.df[\"m3\"] = fp_data.df['PERF_COUNT_HW_INSTRUCTIONS']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m3\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m3\",linewidth=lw)\n",
    "\n",
    "        fp_data.df[\"m4\"] = fp_data.df['MEM_UOPS_RETIRED:ALL_LOADS']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m4\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m4\",linewidth=lw)\n",
    "\n",
    "        fp_data.df[\"m5\"] = fp_data.df['MEM_UOPS_RETIRED:ALL_STORES']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m5\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m5\",linewidth=lw)\n",
    "\n",
    "        fp_data.df[\"m6\"] = fp_data.df['MEM_UOPS_RETIRED:ALL_LOADS']/fp_data.df['PERF_COUNT_HW_INSTRUCTIONS']\n",
    "        fp_data.df = fp_data.df.dropna()\n",
    "        x0, y0 = fp_data.interpolate(feature=\"m6\", npoints=100)\n",
    "        y0 = y0/max(y0)\n",
    "        plot(x0,y0,label=\"m6\",linewidth=lw)\n",
    "\n",
    "        a, b = 0, 1\n",
    "        c, d = ylim()\n",
    "        acc = a\n",
    "        colors = [(0.1,0.1,0.1), (0.4,0.4,0.4)]\n",
    "        for v in zip(x[:-1], x[1:]):\n",
    "            sz = (v[1]-v[0])*(b-a)\n",
    "            bar(acc+sz/2, d-c, sz, c, alpha=0.8, color=colors[-1])\n",
    "            acc += sz\n",
    "            colors = [colors[-1]]+colors[:-1]\n",
    "        ylim(c, d)\n",
    "        \n",
    "        ylabel(\"Normalized fingerprint\")\n",
    "        xlabel(\"Percentage of execution\")\n",
    "        legend()\n",
    "        savefig(f\"phases/metrics_{aname}.pdf\")\n",
    "        show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"databases/best_fps.csv\", index_col=0)\n",
    "df_phases = pd.read_csv(\"databases/phases.csv\", index_col=0)\n",
    "\n",
    "counters_dict = {\n",
    "    \"SYSTEMWIDE:RAPL_ENERGY_PKG\": \"energy_pkg\",\n",
    "    \"SYSTEMWIDE:RAPL_ENERGY_DRAM\": \"energy_ram\",\n",
    "    \"PERF_COUNT_HW_INSTRUCTIONS\": \"instructions\",\n",
    "    \"MEM_UOPS_RETIRED:ALL_STORES\": \"memory_store\",\n",
    "    \"MEM_UOPS_RETIRED:ALL_LOADS\": \"memory_load\",\n",
    "    \"PERF_COUNT_SW_PAGE_FAULTS\": \"page_fault\",\n",
    "    \"PERF_COUNT_SW_CPU_CLOCK\": \"clock\",\n",
    "}\n",
    "\n",
    "matplotlib_rc_1()\n",
    "for _, v in df.iterrows():\n",
    "    fp = v[\"fp\"][1:-1].split(\" \")\n",
    "    fp = [float(x.strip()) for x in fp if x]\n",
    "    fp = np.array(fp)\n",
    "    \n",
    "    dif = np.diff(fp)\n",
    "    dif = np.diff(dif)\n",
    "    dif = abs(dif)\n",
    "    idx = np.where(dif>dif.mean())[0]+1\n",
    "    X = numpy.hstack((idx,fp[idx])).reshape((-1,2),order='F')\n",
    "    \n",
    "    km= KMeans(n_clusters=4-2, random_state=0).fit(X)\n",
    "    centers= km.cluster_centers_\n",
    "    centers= np.vstack( (centers, [0, fp[0]]) )\n",
    "    centers= np.vstack( (centers, [99, fp[99]]) )\n",
    "    centers= centers[centers[:,0].argsort()]\n",
    "    \n",
    "    aname = v[\"app\"]\n",
    "    #print(v[\"app\"], v[\"operations\"], v[\"counters\"])\n",
    "    aa = [counters_dict[x.replace(\"'\",\"\").strip()] for x in v[\"counters\"][1:-1].split(\",\")]\n",
    "    bb = [x.replace(\"'\",\"\").strip() for x in v[\"operations\"][1:-1].split(\",\")]\n",
    "    cc = aa[0]+\"\".join([\"\".join(x) for x in zip(bb,aa[1:])])\n",
    "    print(v[\"app\"], cc)\n",
    "    \n",
    "    xx = df_phases[df_phases[\"app\"].str.contains(aname)]\n",
    "    xx = xx[xx[\"nphases\"] == 4]\n",
    "    xx = xx[\"phases\"].values[-1]\n",
    "    xx = json.loads(xx.replace(\"'\", \"\\\"\"))\n",
    "    x = [float(y)*100 for y in xx]\n",
    "    \n",
    "    title(v[\"app\"]+\": \"+cc)\n",
    "    plot(centers[:,0], centers[:,1])\n",
    "    scatter(centers[:,0], centers[:,1],label=\"critical points\")\n",
    "    plot(fp, label=\"fingerprint\")\n",
    "    \n",
    "    a, b = 0, 1\n",
    "    c, d = ylim()\n",
    "    acc = a\n",
    "    colors = [(0.1,0.1,0.1), (0.4,0.4,0.4)]\n",
    "    for v in zip(x[:-1], x[1:]):\n",
    "        sz = (v[1]-v[0])*(b-a)\n",
    "        bar(acc+sz/2, d-c, sz, c, alpha=0.8, color=colors[-1])\n",
    "        acc += sz\n",
    "        colors = [colors[-1]]+colors[:-1]\n",
    "    ylim(c, d)\n",
    "    \n",
    "    xlabel(\"Normalized time\")\n",
    "    ylabel(\"Fingerprint\")\n",
    "    legend()\n",
    "    tight_layout()\n",
    "    savefig(f\"phases/fingerprint/{aname}.pdf\")\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "matplotlib_rc_2()\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.pkg\": \"./rtview\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        if len(df[\"frequency\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "            \n",
    "            \n",
    "            df_pw = df[df[\"sensors\"] == \"ipmi\"]\n",
    "            df = df[df[\"sensors\"] == \"fingerprint_sample\"]\n",
    "            df[\"info\"] = df[\"info\"].apply(flat)\n",
    "            df = df[df[\"repetitions\"] == \"2\"]\n",
    "            \n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            print(fname)\n",
    "            if not \"rtview\" in fname: continue\n",
    "            print(data.config)\n",
    "            for freq in df[\"frequency\"].values:\n",
    "                y = df[df[\"frequency\"]==freq][\"info\"].values\n",
    "                y = list(y[0])\n",
    "                y = np.diff(y)\n",
    "\n",
    "                if len(y) > 3:\n",
    "                    sz = 1\n",
    "                    if False:\n",
    "                        sz = len(y)\n",
    "                    npoints = 100\n",
    "                    x0, y0= np.linspace(0,sz,len(y)), y\n",
    "                    tck = interpolate.splrep(x0, y0, s=0)\n",
    "                    x1 = np.linspace(0,sz,npoints)\n",
    "                    y1 = interpolate.splev(x1, tck, der=0)\n",
    "                    y = savgol_filter(y1,11,3)\n",
    "                    xlabel(\"Time\")\n",
    "                    ylabel(\"Instructions/seconds\")\n",
    "                    y1 = y1*2.2/freq\n",
    "                    plot(x1, y1, label=freq)\n",
    "\n",
    "            legend()\n",
    "            tight_layout()\n",
    "            show()\n",
    "            \n",
    "            figure(figsize=(15,10))\n",
    "            #display(df_pw)\n",
    "            df_pw = df_pw[df_pw[\"repetitions\"] == \"2\"]\n",
    "            for freq in df_pw[\"frequency\"].values:\n",
    "                y = df_pw[df_pw[\"frequency\"]==freq][\"info\"].values[0]\n",
    "                #y = list(y[0])\n",
    "                #y = np.diff(y)\n",
    "\n",
    "                if len(y) > 3:\n",
    "                    sz = 1\n",
    "                    if False:\n",
    "                        sz = len(y)\n",
    "                    npoints = 100\n",
    "                    x0, y0= np.linspace(0,sz,len(y)), y\n",
    "                    tck = interpolate.splrep(x0, y0, s=0)\n",
    "                    x1 = np.linspace(0,sz,npoints)\n",
    "                    y1 = interpolate.splev(x1, tck, der=0)\n",
    "                    y = savgol_filter(y1,11,3)\n",
    "                    xlabel(\"Time\")\n",
    "                    ylabel(\"Power (W)\")\n",
    "                    #y1 = y1*2.2/freq\n",
    "                    plot(x1, y1, label=freq)\n",
    "            legend()\n",
    "            tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation instructions power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_col = energydb[\"run\"]\n",
    "matplotlib_rc_2()\n",
    "avr_corr = []\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "#         {\n",
    "#             \"$match\":{\n",
    "#                 \"config.pkg\": \"./rtview\"\n",
    "#             }\n",
    "#         },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        \n",
    "        df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "\n",
    "\n",
    "        df_pw = df[df[\"sensors\"] == \"ipmi\"]\n",
    "        df_pw = df_pw[df_pw[\"repetitions\"] == \"2\"]\n",
    "\n",
    "        df = df[df[\"sensors\"] == \"fingerprint_sample\"]\n",
    "        # Grab the first of the sampling group\n",
    "        # For run 4 [INSTRUCTIONS_RETIRED, MEM_UOPS_RETIRED:ALL_LOADS, MEM_UOPS_RETIRED:ALL_STORES]\n",
    "        df[\"info\"] = df[\"info\"].apply(lambda ss: [aux[0] for aux in ss])\n",
    "        df = df[df[\"repetitions\"] == \"2\"]\n",
    "\n",
    "\n",
    "        df = df.rename(columns={\"info\":\"fp\",\"time\":\"fp_time\"})\n",
    "        df = df.drop(columns=\"sensors\")\n",
    "        df_pw = df_pw.rename(columns={\"info\":\"pw\",\"time\":\"pw_time\"})\n",
    "        df_pw = df_pw.drop(columns=\"sensors\")\n",
    "        df = pd.merge(df,df_pw,on=[\"cores\",\"frequency\",\"input\",\"repetitions\"])\n",
    "        #display(df.head(5))\n",
    "\n",
    "        fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "        \n",
    "        # Taking only one row of the dataframe\n",
    "        fp = df[\"fp\"].values[0]\n",
    "        pw = df[\"pw\"].values[0]\n",
    "        fp = np.diff(fp,prepend=[0])\n",
    "        \n",
    "        if len(pw) < 3 or len(fp) < 3:\n",
    "            continue\n",
    "        \n",
    "        print(fname,data.config)\n",
    "        \n",
    "        npoints = 1000\n",
    "        x_fp, y_fp= np.linspace(0,1,len(fp)), fp\n",
    "        tck = interpolate.splrep(x_fp, y_fp, s=0)\n",
    "        x1 = np.linspace(0,1,npoints)\n",
    "        y1 = interpolate.splev(x1, tck, der=0)\n",
    "        \n",
    "        x_pw, y_pw= np.linspace(0,1,len(pw)), pw\n",
    "        tck = interpolate.splrep(x_pw, y_pw, s=0)\n",
    "        x2 = np.linspace(0,1,npoints)\n",
    "        y2 = interpolate.splev(x2, tck, der=0)\n",
    "        \n",
    "        \n",
    "        fig, ax_f = plt.subplots(figsize=(15,10))\n",
    "        ax_c = ax_f.twinx()\n",
    "        ax_f.scatter(x1,y1,c=\"blue\")\n",
    "        ax_c.scatter(x2,y2,c=\"red\")\n",
    "        \n",
    "        \n",
    "        ax_f.set_ylabel(\"Inst/sec\",color=\"blue\", fontsize=24)\n",
    "        ax_c.set_ylabel(\"Power (W)\",color=\"red\", fontsize=24)\n",
    "        ax_f.set_xlabel(\"Time (%)\", fontsize=24)\n",
    "        show()\n",
    "        \n",
    "        figure(figsize=(15,10))\n",
    "        scatter(y1,y2)\n",
    "        avr_corr.append(np.corrcoef(y1,y2)[0][1])\n",
    "        print(\"Correlation\", avr_corr[-1])\n",
    "        ylabel(\"Power (W)\", fontsize=24)\n",
    "        xlabel(\"Inst/sec\", fontsize=24)\n",
    "        show()\n",
    "print(np.mean(avr_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application model from fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "energydb = client[\"energy\"]\n",
    "run_col = energydb[\"run\"]\n",
    "\n",
    "matplotlib_rc_2()\n",
    "for d in run_col.aggregate([\n",
    "        {\n",
    "            \"$match\":{\n",
    "                \"config.data_descriptor.values\": [\"start_time\", \"stop_time\", \"fingerprint_sample\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$match\": { \"config.pkg\": \"./rtview\" },\n",
    "        },\n",
    "        {\n",
    "            \"$group\":\n",
    "            {\n",
    "                \"_id\" : {\"config\":{\"arguments\":\"$config.arguments\"}},\n",
    "                \"ids\": {\"$addToSet\": \"$_id\" },\n",
    "                \"nitem\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ], allowDiskUse=True):\n",
    "    for k in d[\"ids\"]:\n",
    "        if not os.path.isfile(f\"cache/{k}.json\"):\n",
    "            data = run_col.find_one( {\"_id\": ObjectId(k) } )\n",
    "            data[\"_id\"]= str(data[\"_id\"])\n",
    "            json.dump(data, open(f\"cache/{k}.json\", \"w+\"))\n",
    "        data = PascalData(f\"cache/{k}.json\")\n",
    "        df = data.dataframe_group(\"sensors\")\n",
    "        df2 = data.dataframe_generic()\n",
    "        if len(df[\"frequency\"].unique()) > 1 and \"3\" in data.config[\"command\"]:\n",
    "            figure(figsize=(15,10))\n",
    "            \n",
    "            df[\"frequency\"] = df[\"frequency\"].astype(float)/1e6\n",
    "            df = df[df[\"sensors\"] == \"fingerprint_sample\"]\n",
    "            df[\"info\"] = df[\"info\"].apply(flat)\n",
    "            df = df[df[\"repetitions\"] == \"2\"]\n",
    "            df = df[df[\"frequency\"] != \"2300000\"]\n",
    "            \n",
    "            fname = data.config[\"pkg\"][2:]#.capitalize()\n",
    "            print(fname)\n",
    "            #print(data.config)\n",
    "            #df[\"ck\"] = df[\"info\"].apply(np.diff)\n",
    "            #df[\"ck\"] = df[\"ck\"].apply(np.mean)\n",
    "            df[\"T\"] = df[\"time\"].apply(lambda x : x[-1]-x[0])\n",
    "            df[\"I\"] = df[\"info\"].apply(lambda x : x[-1])\n",
    "            df[\"a\"] = df[\"I\"]/(df[\"T\"]*df[\"frequency\"])\n",
    "            a = df[\"a\"].mean()/1e6\n",
    "            I = df[\"I\"].mean()\n",
    "            print(a)\n",
    "            #display(df)\n",
    "            \n",
    "            df2[\"time\"] = df2[\"stop_time\"]-df2[\"start_time\"]\n",
    "            df2 = df2[df2[\"frequency\"] != \"2300000\"]\n",
    "            df2 = df2.groupby(\"frequency\").mean().reset_index()\n",
    "            plot(df2[\"frequency\"].astype(float),df2[\"time\"])\n",
    "            \n",
    "            x1 = np.arange(1200000, 2300000, 100000)\n",
    "            y1 = I/(a*x1)\n",
    "            #y1 = df[\"T\"].max()/x1*1e6*1.2\n",
    "            plot(x1, y1, label=\"aproxx\")\n",
    "            legend()\n",
    "            show()\n",
    "            #display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU  Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_rc_1()\n",
    "df_requests = pd.read_csv(\"databases/cpu_requests.csv\",sep=\" \")\n",
    "df_requests = df_requests[df_requests[\"NCPUS\"] < 1024]\n",
    "df_requests = df_requests[df_requests[\"State\"]==\"COMPLETED\"]\n",
    "df_requests.groupby(\"NCPUS\").count()[\"State\"].sort_values(ascending=False).plot.bar(logy=True)\n",
    "xlabel(\"Requested cores\")\n",
    "ylabel(\"Number of jobs\")\n",
    "savefig(\"experiments/cpu_requestes.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
